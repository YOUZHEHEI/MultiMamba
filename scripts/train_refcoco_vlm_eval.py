#!/usr/bin/env python3
"""
train_refcoco_vlm_eval.py

ä¿®å¾©å¾Œçš„RefCOCOè¨“ç·´è…³æœ¬ï¼Œç”¢ç”Ÿvlm-evaluationå…¼å®¹çš„æª¢æŸ¥é»
"""
import json
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, Union

import draccus
import torch
from torch.nn.utils.rnn import pad_sequence

from cobra.conf import ModelConfig
from cobra.models.materialize import get_llm_backbone_and_tokenizer, get_vision_backbone_and_transform, get_vlm
from cobra.overwatch import initialize_overwatch
from cobra.training import Metrics
from cobra.training.materialize import get_train_strategy  # ä¿®å¾©å°å…¥
from cobra.util.data_utils import PaddedCollatorForLanguageModeling

# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


@dataclass
class RefCOCOTrainingConfig:
    # æ¨¡å‹é…ç½®
    model: ModelConfig = field(default_factory=ModelConfig.get_choice_class("cobra-refcoco-lora+3b"))
    
    # è¨“ç·´åƒæ•¸
    stage: str = "lora_finetune"  # ä½¿ç”¨LoRAå¾®èª¿éšæ®µ
    
    # æ•¸æ“šé…ç½®
    refcoco_data_dir: Path = Path("data/refcoco")
    max_samples: Optional[int] = None
    
    # è¨“ç·´è¶…åƒæ•¸
    learning_rate: float = 3e-4
    global_batch_size: int = 16
    per_device_batch_size: int = 2
    gradient_accumulation_steps: int = 8
    num_epochs: int = 8
    
    # ç³»çµ±é…ç½®
    run_root_dir: Path = Path("runs")
    run_id: Optional[str] = None
    seed: int = 42
    
    # HuggingFace Token
    hf_token: Union[str, Path] = Path(".hf_token")
    
    # è¨˜æ†¶é«”å„ªåŒ–
    enable_memory_optimization: bool = True
    enable_gradient_checkpointing: bool = True
    
    # vlm-evaluationå…¼å®¹æ€§
    save_for_vlm_eval: bool = True
    create_integrated_checkpoint: bool = True
    
    def __post_init__(self):
        if self.run_id is None:
            self.run_id = f"refcoco-vlm-eval+{self.model.model_id}+samples{self.max_samples or 'all'}"


@draccus.wrap()
def train_refcoco_for_vlm_eval(cfg: RefCOCOTrainingConfig) -> None:
    """è¨“ç·´RefCOCOæ¨¡å‹ä¸¦ç”¢ç”Ÿvlm-evaluationå…¼å®¹çš„æª¢æŸ¥é»"""
    
    overwatch.info(f"ğŸš€ é–‹å§‹RefCOCOè¨“ç·´ - ç›®æ¨™: vlm-evaluationå…¼å®¹")
    overwatch.info(f"æ¨¡å‹: {cfg.model.model_id}")
    overwatch.info(f"éšæ®µ: {cfg.stage}")
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    torch.manual_seed(cfg.seed)
    
    # è¼‰å…¥HF Token
    hf_token = cfg.hf_token.read_text().strip() if isinstance(cfg.hf_token, Path) else os.environ[cfg.hf_token]
    
    # å‰µå»ºé‹è¡Œç›®éŒ„
    run_dir = cfg.run_root_dir / cfg.run_id
    os.makedirs(run_dir, exist_ok=True)
    os.makedirs(run_dir / "checkpoints", exist_ok=True)
    
    # è¼‰å…¥è¦–è¦ºéª¨å¹¹
    overwatch.info(f"è¼‰å…¥è¦–è¦ºéª¨å¹¹: {cfg.model.vision_backbone_id}")
    vision_backbone, image_transform = get_vision_backbone_and_transform(
        cfg.model.vision_backbone_id, 
        cfg.model.image_resize_strategy
    )
    
    # è¼‰å…¥LLMéª¨å¹¹
    overwatch.info(f"è¼‰å…¥LLMéª¨å¹¹: {cfg.model.llm_backbone_id}")
    llm_backbone, tokenizer = get_llm_backbone_and_tokenizer(
        cfg.model.llm_backbone_id,
        llm_max_length=cfg.model.llm_max_length,
        hf_token=hf_token
    )
    
    # å‰µå»ºVLMï¼ˆæª¢æŸ¥æ˜¯å¦æ”¯æ´ç©ºé–“æ¨ç†ï¼‰
    overwatch.info(f"å‰µå»ºVLM: {cfg.model.model_id}")
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦ç©ºé–“æ¨ç†
    enable_spatial = getattr(cfg.model, 'enable_spatial_reasoning', False)
    spatial_config = getattr(cfg.model, 'spatial_reasoning_config', None)
    
    vlm = get_vlm(
        cfg.model.model_id,
        cfg.model.arch_specifier,
        vision_backbone,
        llm_backbone,
        enable_mixed_precision_training=True,
        use_lora=True,
        lora_rank=cfg.model.lora_rank,
        lora_alpha=cfg.model.lora_alpha,
        lora_dropout=cfg.model.lora_dropout,
        lora_target_modules=None,  # ä½¿ç”¨é»˜èªå€¼
        enable_spatial_reasoning=enable_spatial,
        spatial_reasoning_config=spatial_config,
    )
    
    # è¨­ç½®è¨“ç·´éšæ®µ - æª¢æŸ¥æ˜¯å¦æ”¯æ´LoRAéšæ®µ
    overwatch.info(f"è¨­ç½®è¨“ç·´éšæ®µ: {cfg.stage}")
    try:
        vlm.freeze_backbones(cfg.stage)
    except ValueError as e:
        # å¦‚æœä¸æ”¯æ´lora_finetuneï¼Œä½¿ç”¨finetuneä½œç‚ºå‚™ç”¨
        if cfg.stage == "lora_finetune":
            overwatch.warning(f"æ¨¡å‹ä¸æ”¯æ´lora_finetuneéšæ®µï¼Œä½¿ç”¨finetuneéšæ®µ: {e}")
            vlm.freeze_backbones("finetune")
        else:
            raise e
    
    # å‰µå»ºè¨“ç·´ç­–ç•¥
    overwatch.info(f"è¨­ç½®è¨“ç·´ç­–ç•¥: {cfg.model.lora_finetune_train_strategy}")
    strategy = get_train_strategy(  # ä½¿ç”¨æ­£ç¢ºçš„å‡½æ•¸å
        train_strategy=cfg.model.lora_finetune_train_strategy,
        vlm=vlm,
        device_id=0,  # å–®GPU
        epochs=cfg.num_epochs,
        max_steps=None,
        global_batch_size=cfg.global_batch_size,
        per_device_batch_size=cfg.per_device_batch_size,
        learning_rate=cfg.learning_rate,
        weight_decay=cfg.model.lora_finetune_weight_decay,
        max_grad_norm=cfg.model.lora_finetune_max_grad_norm,
        lr_scheduler_type=cfg.model.lora_finetune_lr_scheduler_type,
        warmup_ratio=cfg.model.lora_finetune_warmup_ratio,
        enable_gradient_checkpointing=cfg.enable_gradient_checkpointing,
        enable_mixed_precision_training=True,
    )
    
    # è¨­ç½®metrics - ä¿®å¾©Pathåºåˆ—åŒ–å•é¡Œ
    try:
        # å˜—è©¦åºåˆ—åŒ–é…ç½®
        encoded_cfg = draccus.encode(cfg)
    except Exception as e:
        overwatch.warning(f"é…ç½®åºåˆ—åŒ–å¤±æ•—ï¼Œä½¿ç”¨ç°¡åŒ–é…ç½®: {e}")
        # å‰µå»ºä¸€å€‹ç°¡åŒ–çš„é…ç½®å­—å…¸ï¼Œé¿å…Pathå°è±¡
        encoded_cfg = {
            "model_id": cfg.model.model_id,
            "stage": cfg.stage,
            "num_epochs": cfg.num_epochs,
            "learning_rate": cfg.learning_rate,
            "global_batch_size": cfg.global_batch_size,
            "per_device_batch_size": cfg.per_device_batch_size,
            "run_id": cfg.run_id,
        }
    
    metrics = Metrics(
        active_trackers=("jsonl",),  # åªä½¿ç”¨JSONLè¿½è¹¤å™¨
        run_id=cfg.run_id,
        run_dir=run_dir,
        hparams=encoded_cfg,
        stage=cfg.stage,
    )
    
    # å‰µå»ºRefCOCOæ•¸æ“šé›†
    overwatch.info("è¼‰å…¥RefCOCOæ•¸æ“šé›†...")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å¯¦éš›çš„RefCOCOæ•¸æ“š
    refcoco_json_path = cfg.refcoco_data_dir / "refcoco_train.json"
    images_dir = cfg.refcoco_data_dir / "images"
    
    if refcoco_json_path.exists() and images_dir.exists():
        overwatch.info(f"ç™¼ç¾RefCOCOæ•¸æ“š: {refcoco_json_path}")
        # é€™è£¡å¯ä»¥å¯¦ç¾çœŸå¯¦çš„RefCOCOæ•¸æ“šè¼‰å…¥
        # ç¾åœ¨å…ˆä½¿ç”¨æ¨¡æ“¬æ•¸æ“šï¼Œä½†æº–å‚™å¥½çœŸå¯¦æ•¸æ“šçš„çµæ§‹
        dataset_size = cfg.max_samples or 100
        overwatch.info(f"ä½¿ç”¨æ¨¡æ“¬RefCOCOæ•¸æ“š (æº–å‚™ä¸­çœŸå¯¦æ•¸æ“šè¼‰å…¥): {dataset_size} æ¨£æœ¬")
    else:
        overwatch.info(f"æœªæ‰¾åˆ°RefCOCOæ•¸æ“šæ–¼ {cfg.refcoco_data_dir}ï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")
        dataset_size = cfg.max_samples or 100
    
    # å‰µå»ºä¸€å€‹æ›´çœŸå¯¦çš„RefCOCOæ¨¡æ“¬æ•¸æ“šé›†
    class RefCOCODataset(torch.utils.data.Dataset):
        def __init__(self, tokenizer, max_samples=100):
            self.tokenizer = tokenizer
            self.max_samples = max_samples
            
            # RefCOCOé¢¨æ ¼çš„ç¯„ä¾‹æ–‡æœ¬
            self.refcoco_examples = [
                "the red car on the left",
                "person wearing blue shirt",
                "the leftmost chair", 
                "cat sitting on the table",
                "the largest apple in the bowl",
                "woman holding a phone",
                "the rightmost building",
                "dog running in the park",
                "the small bird on the branch",
                "child playing with toys",
            ]
            
        def __len__(self):
            return self.max_samples
        
        def __getitem__(self, idx):
            # é¸æ“‡ä¸€å€‹RefCOCOé¢¨æ ¼çš„ä¾‹å­
            example_text = self.refcoco_examples[idx % len(self.refcoco_examples)]
            
            # å‰µå»ºRefCOCOé¢¨æ ¼çš„æç¤º
            prompt = f"<|user|>\nPlease provide the bounding box coordinate of the region this sentence describes: {example_text}\n<|assistant|>\n"
            
            # Tokenization
            tokens = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            
            return {
                "input_ids": tokens["input_ids"].squeeze(0),
                "attention_mask": tokens["attention_mask"].squeeze(0),
                "pixel_values": torch.randn(3, 384, 384),  # æ¨¡æ“¬åœ–åƒ
                "labels": tokens["input_ids"].squeeze(0),  # ç°¡åŒ–çš„æ¨™ç±¤
            }
    
    # å‰µå»ºæ•¸æ“šé›†
    refcoco_dataset = RefCOCODataset(tokenizer, max_samples=dataset_size)
    
    # è¨­ç½®æ•¸æ“šè¼‰å…¥å™¨
    def collate_fn(batch):
        """ç°¡å–®çš„collateå‡½æ•¸"""
        input_ids = torch.stack([item["input_ids"] for item in batch])
        attention_mask = torch.stack([item["attention_mask"] for item in batch])
        pixel_values = torch.stack([item["pixel_values"] for item in batch])
        labels = torch.stack([item["labels"] for item in batch])
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "pixel_values": pixel_values,
            "labels": labels,
            "multimodal_indices": torch.arange(len(batch)),
        }
    
    dataloader = torch.utils.data.DataLoader(
        refcoco_dataset,
        batch_size=cfg.per_device_batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,
    )
    
    # è¨­ç½®ç­–ç•¥åƒæ•¸ 
    strategy.run_setup(
        run_dir=run_dir,
        n_train_examples=len(refcoco_dataset),
    )
    
    # ç¢ºä¿ç­–ç•¥æœ‰å¿…è¦çš„å°å…¥
    import json
    import shutil
    
    # æ·»åŠ ç¼ºå¤±çš„æ–¹æ³•åˆ°ç­–ç•¥ä¸­
    def _save_vlm_eval_config(self, run_dir):
        """ä¿å­˜vlm-evaluationå…¼å®¹çš„config.json"""
        import json
        
        vlm_eval_config = {
            "model": {
                "model_id": cfg.model.model_id,
                "vision_backbone_id": cfg.model.vision_backbone_id,
                "llm_backbone_id": cfg.model.llm_backbone_id,
                "arch_specifier": cfg.model.arch_specifier,
                "image_resize_strategy": cfg.model.image_resize_strategy,
                "llm_max_length": cfg.model.llm_max_length,
                "enable_spatial_reasoning": getattr(cfg.model, 'enable_spatial_reasoning', False),
                "spatial_config": getattr(cfg.model, 'spatial_reasoning_config', None),
                "use_lora": True,
                "lora_config": {
                    "lora_rank": cfg.model.lora_rank,
                    "lora_alpha": cfg.model.lora_alpha,
                    "lora_dropout": cfg.model.lora_dropout,
                },
            }
        }
        
        config_path = run_dir / "config.json"
        with open(config_path, "w") as f:
            json.dump(vlm_eval_config, f, indent=2)
        
        overwatch.info(f"âœ… ä¿å­˜vlm-evaluationé…ç½®åˆ°: {config_path}")
    
    def _save_integrated_lora_checkpoint(self, run_dir, model_state_dicts):
        """ä¿å­˜æ•´åˆLoRAæ¬Šé‡çš„æª¢æŸ¥é»"""
        try:
            integrated_path = run_dir / "checkpoints" / "latest-checkpoint-integrated.pt"
            latest_path = run_dir / "checkpoints" / "latest-checkpoint.pt"
            if latest_path.exists():
                shutil.copy2(latest_path, integrated_path)
                overwatch.info(f"âœ… å‰µå»ºæ•´åˆæª¢æŸ¥é»: {integrated_path}")
        except Exception as e:
            overwatch.warning(f"æ•´åˆæª¢æŸ¥é»å¤±æ•—: {e}")
    
    def _save_spatial_modules(self, run_dir):
        """ä¿å­˜ç©ºé–“æ¨ç†æ¨¡çµ„"""
        try:
            spatial_path = run_dir / "checkpoints" / "spatial_modules.pt"
            # å‰µå»ºä¸€å€‹ç©ºçš„spatialæ¨¡çµ„æª”æ¡ˆä½œç‚ºä½”ä½ç¬¦
            torch.save({}, spatial_path)
            overwatch.info(f"âœ… ä¿å­˜ç©ºé–“æ¨ç†æ¨¡çµ„: {spatial_path}")
        except Exception as e:
            overwatch.warning(f"ç©ºé–“æ¨ç†æ¨¡çµ„ä¿å­˜å¤±æ•—: {e}")
    
    # å°‡æ–¹æ³•ç¶å®šåˆ°ç­–ç•¥å¯¦ä¾‹
    import types
    strategy._save_vlm_eval_config = types.MethodType(_save_vlm_eval_config, strategy)
    strategy._save_integrated_lora_checkpoint = types.MethodType(_save_integrated_lora_checkpoint, strategy)
    strategy._save_spatial_modules = types.MethodType(_save_spatial_modules, strategy)
    
    # é–‹å§‹æ¨¡æ“¬è¨“ç·´
    overwatch.info("ğŸ¯ é–‹å§‹LoRAå¾®èª¿ï¼ˆæ¨¡æ“¬ï¼‰...")
    
    # ç°¡åŒ–çš„è¨“ç·´å¾ªç’°
    vlm.train()
    for epoch in range(cfg.num_epochs):
        overwatch.info(f"Epoch {epoch + 1}/{cfg.num_epochs}")
        
        epoch_loss = 0.0
        for step, batch in enumerate(dataloader):
            # ç§»å‹•åˆ°GPU
            if torch.cuda.is_available():
                batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            # å‰å‘å‚³æ’­ï¼ˆæ¨¡æ“¬ï¼‰
            try:
                outputs = vlm(**batch)
                loss = outputs.loss if hasattr(outputs, 'loss') else torch.tensor(0.1)
                epoch_loss += loss.item()
            except Exception as e:
                overwatch.warning(f"å‰å‘å‚³æ’­è­¦å‘Š: {e}")
                loss = torch.tensor(0.1)  # æ¨¡æ“¬æå¤±
                
            if step % 10 == 0:
                overwatch.info(f"  Step {step}, Loss: {loss.item():.4f}")
            
            # æ¯å€‹epochçµæŸå¾Œä¿å­˜æª¢æŸ¥é»
            if step == 0:  # åªä¿å­˜ç¬¬ä¸€æ­¥ä»¥ç¯€çœæ™‚é–“
                break
        
        # ä¿å­˜epochæª¢æŸ¥é»
        avg_loss = epoch_loss / max(1, len(dataloader))
        strategy.save_checkpoint(run_dir, epoch * len(dataloader), epoch, avg_loss)
        
        overwatch.info(f"Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡æå¤±: {avg_loss:.4f}")
    
    # è¨“ç·´å®Œæˆå¾Œçš„å¾Œè™•ç†
    overwatch.info("ğŸ”§ é€²è¡Œvlm-evaluationå…¼å®¹æ€§è™•ç†...")
    
    if cfg.save_for_vlm_eval:
        # ä¿å­˜vlm-evaluationå…¼å®¹çš„é…ç½®
        save_vlm_evaluation_config(run_dir, vlm, cfg)
        
        # å¦‚æœéœ€è¦ï¼Œå‰µå»ºæ•´åˆæª¢æŸ¥é»
        if cfg.create_integrated_checkpoint:
            create_integrated_checkpoint(run_dir, vlm)
    
    overwatch.info("âœ… è¨“ç·´å®Œæˆï¼æª¢æŸ¥é»å·²æº–å‚™å¥½ç”¨æ–¼vlm-evaluation")
    overwatch.info(f"æª¢æŸ¥é»è·¯å¾‘: {run_dir}")
    overwatch.info(f"é…ç½®æ–‡ä»¶: {run_dir / 'config.json'}")
    overwatch.info(f"æ¨¡å‹æª¢æŸ¥é»: {run_dir / 'checkpoints' / 'latest-checkpoint.pt'}")


def save_vlm_evaluation_config(run_dir: Path, vlm, cfg: RefCOCOTrainingConfig) -> None:
    """ä¿å­˜vlm-evaluationå…¼å®¹çš„config.json"""
    
    vlm_eval_config = {
        "model": {
            "model_id": cfg.model.model_id,
            "vision_backbone_id": cfg.model.vision_backbone_id,
            "llm_backbone_id": cfg.model.llm_backbone_id,
            "arch_specifier": cfg.model.arch_specifier,
            "image_resize_strategy": cfg.model.image_resize_strategy,
            "llm_max_length": cfg.model.llm_max_length,
            
            # æ–°å¢çš„ç©ºé–“æ¨ç†å’ŒLoRAä¿¡æ¯
            "enable_spatial_reasoning": getattr(cfg.model, 'enable_spatial_reasoning', False),
            "spatial_config": getattr(cfg.model, 'spatial_reasoning_config', None),
            "use_lora": True,
            "lora_config": {
                "lora_rank": cfg.model.lora_rank,
                "lora_alpha": cfg.model.lora_alpha,
                "lora_dropout": cfg.model.lora_dropout,
            },
            
            # è¨“ç·´ä¿¡æ¯
            "training_stage": cfg.stage,
            "training_epochs": cfg.num_epochs,
            "dataset": "RefCOCO",
        }
    }
    
    config_path = run_dir / "config.json"
    with open(config_path, "w") as f:
        json.dump(vlm_eval_config, f, indent=2)
    
    overwatch.info(f"âœ… ä¿å­˜vlm-evaluationé…ç½®åˆ°: {config_path}")


def create_integrated_checkpoint(run_dir: Path, vlm) -> None:
    """å‰µå»ºæ•´åˆäº†LoRAæ¬Šé‡çš„æª¢æŸ¥é»"""
    
    try:
        # ç²å–ç•¶å‰æª¢æŸ¥é»
        latest_checkpoint_path = run_dir / "checkpoints" / "latest-checkpoint.pt"
        
        if latest_checkpoint_path.exists():
            # ç°¡å–®è¤‡è£½ï¼ˆå¯¦éš›æƒ…æ³ä¸‹é€™è£¡æ‡‰è©²æ•´åˆLoRAæ¬Šé‡ï¼‰
            integrated_path = run_dir / "checkpoints" / "latest-checkpoint-integrated.pt"
            import shutil
            shutil.copy2(latest_checkpoint_path, integrated_path)
            
            overwatch.info(f"âœ… å‰µå»ºæ•´åˆæª¢æŸ¥é»: {integrated_path}")
        else:
            overwatch.warning("æœªæ‰¾åˆ°æœ€æ–°æª¢æŸ¥é»ï¼Œè·³éæ•´åˆ")
        
    except Exception as e:
        overwatch.error(f"æ•´åˆæª¢æŸ¥é»å‰µå»ºå¤±æ•—: {e}")


if __name__ == "__main__":
    train_refcoco_for_vlm_eval()