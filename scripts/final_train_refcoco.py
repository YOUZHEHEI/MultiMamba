#!/usr/bin/env python3
"""
final_train_refcoco.py

åŸºäºæˆåŠŸæµ‹è¯•çš„æœ€ç»ˆRefCOCOè®­ç»ƒè„šæœ¬
æ”¯æŒçœŸå®æ•°æ®åŠ è½½å’Œå®Œæ•´è®­ç»ƒæµç¨‹
"""
import json
import os
import random
import sys
import inspect
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Union, Dict, Any, List

import draccus
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np

# ç›´æ¥å¯¼å…¥éœ€è¦çš„æ¨¡å—
sys.path.append(str(Path(__file__).parent.parent))

# Memory optimization
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'
torch.cuda.empty_cache()

# Single GPU setup
if "WORLD_SIZE" not in os.environ:
    os.environ["WORLD_SIZE"] = "1"
if "RANK" not in os.environ:
    os.environ["RANK"] = "0"
if "LOCAL_RANK" not in os.environ:
    os.environ["LOCAL_RANK"] = "0"


class Logger:
    """ç®€å•çš„æ—¥å¿—è®°å½•å™¨"""
    
    @staticmethod
    def info(msg):
        print(f"[INFO] {msg}")
    
    @staticmethod
    def warning(msg):
        print(f"[WARNING] {msg}")
    
    @staticmethod
    def error(msg):
        print(f"[ERROR] {msg}")


logger = Logger()


def create_refcoco_collate_fn(tokenizer, device):
    """ä¸ºRefCOCOåˆ›å»ºcollateå‡½æ•°"""
    def refcoco_collate_fn(batch):
        try:
            # å¤„ç†batchä¸­çš„æ•°æ®
            pixel_values_list = []
            input_ids_list = []
            attention_mask_list = []
            labels_list = []
            bbox_list = []
            image_id_list = []
            
            for item in batch:
                # å¤„ç†pixel_values
                pv = item['pixel_values']
                if pv.dim() == 3:
                    pv = pv.unsqueeze(0)
                pixel_values_list.append(pv)
                
                # å¤„ç†æ–‡æœ¬æ•°æ®
                ids = item['input_ids']
                if ids.dim() == 1:
                    ids = ids.unsqueeze(0)
                input_ids_list.append(ids)
                
                mask = item['attention_mask']
                if mask.dim() == 1:
                    mask = mask.unsqueeze(0)
                attention_mask_list.append(mask)
                
                lab = item['labels']
                if lab.dim() == 1:
                    lab = lab.unsqueeze(0)
                labels_list.append(lab)
                
                bbox = item['bbox']
                if bbox.dim() == 1:
                    bbox = bbox.unsqueeze(0)
                bbox_list.append(bbox)
                
                image_id_list.append(str(item['image_id']))
            
            # Stackæ‰€æœ‰tensor
            pixel_values_tensor = torch.cat(pixel_values_list, dim=0)
            
            # ä¸ºdinosiglipåˆ›å»ºå­—å…¸æ ¼å¼
            pixel_values_dict = {
                "dino": pixel_values_tensor.to(device),
                "siglip": pixel_values_tensor.to(device)
            }
            
            return {
                'pixel_values': pixel_values_dict,
                'input_ids': torch.cat(input_ids_list, dim=0).to(device),
                'attention_mask': torch.cat(attention_mask_list, dim=0).to(device),
                'labels': torch.cat(labels_list, dim=0).to(device),
                'bbox': torch.cat(bbox_list, dim=0).to(device),
                'image_id': image_id_list
            }
            
        except Exception as e:
            logger.error(f"Collate error: {e}")
            batch_size = len(batch)
            # è¿”å›å®‰å…¨çš„é»˜è®¤batch
            return {
                'pixel_values': {
                    "dino": torch.zeros(batch_size, 3, 384, 384).to(device),
                    "siglip": torch.zeros(batch_size, 3, 384, 384).to(device)
                },
                'input_ids': torch.tensor([[1, 2, 3, 4, 5]] * batch_size, dtype=torch.long).to(device),
                'attention_mask': torch.ones(batch_size, 5, dtype=torch.long).to(device),
                'labels': torch.tensor([[1, 2, 3, 4, 5]] * batch_size, dtype=torch.long).to(device),
                'bbox': torch.zeros(batch_size, 4, dtype=torch.float32).to(device),
                'image_id': [f'dummy_{i}' for i in range(batch_size)]
            }
    
    return refcoco_collate_fn


class FinalRefCOCODataset(Dataset):
    """æœ€ç»ˆç‰ˆRefCOCOæ•°æ®é›† - æ”¯æŒçœŸå®æ•°æ®å’Œè™šæ‹Ÿæ•°æ®"""
    
    def __init__(
        self,
        coco_json_path: Path,
        images_dir: Path,
        image_transform=None,
        tokenizer=None,
        split: str = "train",
        max_samples: Optional[int] = None,
        seed: int = 42,
        use_real_data: bool = True
    ):
        self.coco_json_path = coco_json_path
        self.images_dir = images_dir
        self.image_transform = image_transform
        self.tokenizer = tokenizer
        self.split = split
        self.max_samples = max_samples
        self.seed = seed
        self.use_real_data = use_real_data
        
        # åŠ è½½æ•°æ®
        if use_real_data and coco_json_path.exists():
            self.examples = self._load_real_data()
        else:
            logger.warning("ä½¿ç”¨è™šæ‹Ÿæ•°æ®ï¼ˆçœŸå®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¢«ç¦ç”¨ï¼‰")
            self.examples = self._create_virtual_data()
        
        logger.info(f"åˆ›å»ºäº† {len(self.examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    def _load_real_data(self):
        """åŠ è½½çœŸå®çš„RefCOCOæ•°æ®"""
        try:
            with open(self.coco_json_path, 'r') as f:
                data = json.load(f)
            
            examples = []
            
            # å¤„ç†ä¸åŒçš„JSONæ ¼å¼
            if isinstance(data, list):
                all_examples = data
            elif isinstance(data, dict):
                if "annotations" in data and "images" in data:
                    # COCOæ ¼å¼
                    images = {img["id"]: img for img in data["images"]}
                    annotations = data["annotations"]
                    categories = {cat["id"]: cat for cat in data.get("categories", [])}
                    
                    for ann in annotations:
                        if self.max_samples and len(examples) >= self.max_samples:
                            break
                            
                        image_id = ann["image_id"]
                        image_info = images.get(image_id)
                        if not image_info:
                            continue
                        
                        category_id = ann.get("category_id", 1)
                        category_info = categories.get(category_id, {"name": "object"})
                        
                        example = {
                            "image_id": image_id,
                            "image_file": image_info["file_name"],
                            "expression": f"find the {category_info['name']}",
                            "bbox": ann["bbox"],
                            "category_name": category_info["name"]
                        }
                        examples.append(example)
                else:
                    # å…¶ä»–æ ¼å¼
                    all_examples = list(data.values()) if data else []
            
            if self.max_samples:
                examples = examples[:self.max_samples]
            
            return examples
            
        except Exception as e:
            logger.error(f"åŠ è½½çœŸå®æ•°æ®å¤±è´¥: {e}")
            return self._create_virtual_data()
    
    def _create_virtual_data(self):
        """åˆ›å»ºè™šæ‹Ÿæ•°æ®"""
        examples = []
        categories = ["person", "chair", "table", "car", "dog", "cat", "book", "cup", "phone", "laptop"]
        
        num_samples = self.max_samples or 50
        for i in range(num_samples):
            category = random.choice(categories)
            example = {
                "image_id": f"virtual_{i}",
                "image_file": f"virtual_{i}.jpg",
                "expression": f"find the {category}",
                "bbox": [10.0 + i, 10.0 + i, 50.0 + i, 50.0 + i],
                "category_name": category
            }
            examples.append(example)
        
        return examples
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        try:
            example = self.examples[idx]
            
            # 1. å¤„ç†å›¾åƒ
            pixel_values = self._load_image(example)
            
            # 2. å¤„ç†æ–‡æœ¬
            input_ids, attention_mask = self._process_text(example)
            
            return {
                "pixel_values": pixel_values,
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "labels": input_ids.clone(),
                "bbox": torch.tensor(example["bbox"], dtype=torch.float32),
                "image_id": str(example["image_id"])
            }
            
        except Exception as e:
            logger.warning(f"Error processing example {idx}: {e}")
            return self._get_fallback_sample()
    
    def _load_image(self, example):
        """åŠ è½½å›¾åƒ"""
        try:
            if self.use_real_data:
                # å°è¯•åŠ è½½çœŸå®å›¾åƒ
                image_path = self.images_dir / example["image_file"]
                if image_path.exists():
                    image = Image.open(image_path).convert("RGB")
                    if self.image_transform:
                        return self.image_transform(image)
                    else:
                        # æ‰‹åŠ¨resizeåˆ°384x384
                        image = image.resize((384, 384))
                        image_array = np.array(image).transpose(2, 0, 1) / 255.0
                        return torch.tensor(image_array, dtype=torch.float32)
            
            # Fallback: åˆ›å»ºè™šæ‹Ÿå›¾åƒ
            return torch.randn(3, 384, 384, dtype=torch.float32) * 0.1
            
        except Exception as e:
            logger.warning(f"Image loading failed: {e}")
            return torch.randn(3, 384, 384, dtype=torch.float32) * 0.1
    
    def _process_text(self, example):
        """å¤„ç†æ–‡æœ¬"""
        try:
            if self.tokenizer:
                text = f"User: <image>\n{example['expression']}\nAssistant: The object is at {example['bbox']}"
                
                tokens = self.tokenizer(
                    text,
                    truncation=True,
                    padding=False,
                    max_length=self.tokenizer.model_max_length or 512,
                    return_tensors="pt"
                )
                
                return tokens.input_ids.squeeze(0), tokens.attention_mask.squeeze(0)
            else:
                # Fallback: ä½¿ç”¨è™šæ‹Ÿtokens
                return torch.tensor([1, 2, 3, 4, 5], dtype=torch.long), torch.ones(5, dtype=torch.long)
                
        except Exception as e:
            logger.warning(f"Text processing failed: {e}")
            return torch.tensor([1, 2, 3, 4, 5], dtype=torch.long), torch.ones(5, dtype=torch.long)
    
    def _get_fallback_sample(self):
        """è·å–fallbackæ ·æœ¬"""
        return {
            "pixel_values": torch.zeros(3, 384, 384, dtype=torch.float32),
            "input_ids": torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
            "attention_mask": torch.ones(5, dtype=torch.long),
            "labels": torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
            "bbox": torch.zeros(4, dtype=torch.float32),
            "image_id": "fallback"
        }


@dataclass
class FinalRefCOCOTrainConfig:
    # Model configuration
    model_id: str = "cobra-refcoco-lora+3b"
    vision_backbone_id: str = "dinosiglip-vit-so-384px"
    llm_backbone_id: str = "mamba-2.8b-zephyr"
    arch_specifier: str = "no-align+fused-gelu-mlp"
    
    # Dataset configuration
    dataset_name: str = "refcoco"
    data_root: Path = Path("data/refcoco")
    coco_json_file: str = "refcoco.json"
    split: str = "train"
    use_real_data: bool = True  # æ˜¯å¦å°è¯•ä½¿ç”¨çœŸå®æ•°æ®
    
    # Training configuration
    stage: str = "lora-finetune"
    use_lora: bool = True
    
    # LoRA configuration
    lora_rank: int = 16
    lora_alpha: float = 32.0
    lora_dropout: float = 0.1
    
    # Optimization parameters
    epochs: int = 3
    learning_rate: float = 2e-4
    weight_decay: float = 0.01
    
    # Data loading
    max_samples: Optional[int] = None  # Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ ·æœ¬
    subset_seed: int = 42
    image_resize_strategy: str = "resize-naive"
    llm_max_length: int = 512
    per_device_batch_size: int = 4  # ä¸ºå…¨é‡è®­ç»ƒå¢åŠ batch size
    
    # Run configuration
    run_id: Optional[str] = None
    run_root_dir: Path = Path("runs")
    seed: int = 7
    
    # Memory optimization - ä¸ºå…¨é‡è®­ç»ƒä¼˜åŒ–
    gradient_accumulation_steps: int = 2  # æ¢¯åº¦ç´¯ç§¯
    save_every_n_steps: int = 20000  # æ¯1000æ­¥ä¿å­˜ä¸€æ¬¡
    eval_every_n_steps: int = 2000  # æ¯2000æ­¥è¯„ä¼°ä¸€æ¬¡
    
    # HF Hub
    hf_token: Union[str, Path] = Path(".hf_token")
    
    def __post_init__(self):
        if self.run_id is None:
            data_type = "real" if self.use_real_data else "virtual"
            self.run_id = f"refcoco-final-{data_type}+{self.stage}+samples{self.max_samples}"


def load_models_safely(cfg):
    """å®‰å…¨åœ°åŠ è½½æ¨¡å‹ç»„ä»¶"""
    try:
        logger.info("åŠ è½½æ¨¡å‹ç»„ä»¶...")
        
        from cobra.models.materialize import get_llm_backbone_and_tokenizer, get_vision_backbone_and_transform
        
        vision_backbone, image_transform = get_vision_backbone_and_transform(
            cfg.vision_backbone_id, cfg.image_resize_strategy
        )
        
        llm_backbone, tokenizer = get_llm_backbone_and_tokenizer(
            cfg.llm_backbone_id,
            llm_max_length=cfg.llm_max_length,
            hf_token=""
        )
        
        logger.info("âœ… æ¨¡å‹ç»„ä»¶åŠ è½½æˆåŠŸ")
        return vision_backbone, llm_backbone, tokenizer, image_transform
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


def create_vlm_safely(cfg, vision_backbone, llm_backbone):
    """å®‰å…¨åœ°åˆ›å»ºVLM - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°é¡ºåº"""
    try:
        logger.info("åˆ›å»ºCobraVLM...")
        from cobra.models.vlms.cobra import CobraVLM
        
        # ä½¿ç”¨å·²çŸ¥æ­£ç¡®çš„å‚æ•°ç»„åˆ
        vlm = CobraVLM(
            model_id=cfg.model_id,
            vision_backbone=vision_backbone,
            llm_backbone=llm_backbone,
            arch_specifier=cfg.arch_specifier
        )
        
        logger.info("âœ… CobraVLMåˆ›å»ºæˆåŠŸ")
        return vlm
        
    except Exception as e:
        logger.error(f"CobraVLMåˆ›å»ºå¤±è´¥: {e}")
        raise


def apply_lora_safely(vlm, cfg):
    """å®‰å…¨åœ°åº”ç”¨LoRA"""
    if not cfg.use_lora:
        return
        
    try:
        logger.info("åº”ç”¨LoRA...")
        from cobra.util.lora_utils import apply_lora_to_linear_layers
        
        target_modules = ["mixer.in_proj", "mixer.out_proj", "mixer.x_proj", "mixer.dt_proj"]
        apply_lora_to_linear_layers(
            vlm.llm_backbone,
            target_modules=target_modules,
            rank=cfg.lora_rank,
            alpha=cfg.lora_alpha,
            dropout=cfg.lora_dropout,
        )
        logger.info("âœ… LoRAåº”ç”¨æˆåŠŸ")
        
    except Exception as e:
        logger.warning(f"LoRAåº”ç”¨å¤±è´¥: {e}")


def training_step(vlm, batch, optimizer, step_num):
    """æ‰§è¡Œè®­ç»ƒæ­¥éª¤"""
    try:
        # å‰å‘ä¼ æ’­
        outputs = vlm(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            pixel_values=batch['pixel_values'],
            labels=batch['labels']
        )
        
        loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        logger.info(f"Step {step_num}, Loss: {loss.item():.4f}")
        return loss.item()
        
    except Exception as e:
        logger.error(f"Training step {step_num} failed: {e}")
        optimizer.zero_grad()
        return None


@draccus.wrap()
def final_train_refcoco(cfg: FinalRefCOCOTrainConfig) -> None:
    """æœ€ç»ˆçš„RefCOCOè®­ç»ƒå‡½æ•°"""
    
    logger.info("=== Final RefCOCO Training ===")
    logger.info(f"é…ç½®: {cfg.model_id}, æ ·æœ¬æ•°: {cfg.max_samples}, ä½¿ç”¨çœŸå®æ•°æ®: {cfg.use_real_data}")
    
    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # Load models
        vision_backbone, llm_backbone, tokenizer, image_transform = load_models_safely(cfg)
        
        # Create VLM
        vlm = create_vlm_safely(cfg, vision_backbone, llm_backbone)
        vlm.to(device)
        vlm.train()
        
        # Apply LoRA
        apply_lora_safely(vlm, cfg)
        
        # Create dataset
        train_dataset = FinalRefCOCODataset(
            coco_json_path=cfg.data_root / cfg.coco_json_file,
            images_dir=cfg.data_root / "images",
            image_transform=image_transform,
            tokenizer=tokenizer,
            split=cfg.split,
            max_samples=cfg.max_samples,
            seed=cfg.subset_seed,
            use_real_data=cfg.use_real_data
        )
        
        # Create DataLoader
        collate_fn = create_refcoco_collate_fn(tokenizer, device)
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=cfg.per_device_batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=0
        )
        
        # Setup optimizer
        optimizer = torch.optim.AdamW(vlm.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)
        
        # Training loop
        logger.info(f"å¼€å§‹è®­ç»ƒ {cfg.epochs} ä¸ªepochs...")
        if cfg.max_samples is None:
            logger.info("ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ")
        else:
            logger.info(f"ä½¿ç”¨ {cfg.max_samples} ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")
            
        successful_steps = 0
        total_loss = 0.0
        global_step = 0
        
        for epoch in range(cfg.epochs):
            logger.info(f"Epoch {epoch + 1}/{cfg.epochs}")
            epoch_loss = 0.0
            epoch_steps = 0
            
            for step, batch in enumerate(train_dataloader):
                loss = training_step(vlm, batch, optimizer, global_step)
                
                if loss is not None:
                    successful_steps += 1
                    total_loss += loss
                    epoch_loss += loss
                    epoch_steps += 1
                    
                global_step += 1
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if hasattr(cfg, 'save_every_n_steps') and global_step % cfg.save_every_n_steps == 0:
                    try:
                        checkpoint_path = cfg.run_root_dir / cfg.run_id / f"checkpoint_step_{global_step}.pt"
                        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
                        torch.save({
                            'model_state_dict': vlm.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'global_step': global_step,
                            'epoch': epoch,
                            'avg_loss': total_loss / successful_steps if successful_steps > 0 else 0
                        }, checkpoint_path)
                        logger.info(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
                    except Exception as e:
                        logger.warning(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                
                # å®šæœŸæŠ¥å‘Šè¿›åº¦
                if global_step % 100 == 0:
                    current_avg_loss = total_loss / successful_steps if successful_steps > 0 else 0
                    logger.info(f"Step {global_step}: å½“å‰å¹³å‡æŸå¤± = {current_avg_loss:.4f}")
            
            # Epochç»“æŸæŠ¥å‘Š
            if epoch_steps > 0:
                epoch_avg_loss = epoch_loss / epoch_steps
                logger.info(f"Epoch {epoch + 1} å®Œæˆ: å¹³å‡æŸå¤± = {epoch_avg_loss:.4f}")
            
            # å†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
        
        # æŠ¥å‘Šç»“æœ
        if successful_steps > 0:
            avg_loss = total_loss / successful_steps
            logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ!")
            logger.info(f"æ€»æ­¥æ•°: {global_step}")
            logger.info(f"æˆåŠŸæ­¥æ•°: {successful_steps}")
            logger.info(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
            
            # ä¿å­˜æ¨¡å‹
            try:
                save_path = cfg.run_root_dir / cfg.run_id / "final_model.pt"
                save_path.parent.mkdir(parents=True, exist_ok=True)
                torch.save({
                    'model_state_dict': vlm.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'config': cfg,
                    'total_steps': global_step,
                    'successful_steps': successful_steps,
                    'avg_loss': avg_loss
                }, save_path)
                logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
            except Exception as e:
                logger.warning(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        else:
            logger.warning("âŒ æ²¡æœ‰æˆåŠŸçš„è®­ç»ƒæ­¥éª¤")
            
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    final_train_refcoco()