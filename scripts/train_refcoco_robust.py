#!/usr/bin/env python3
"""
train_refcoco_robust.py

ç©©å¥çš„RefCOCOè¨“ç·´è…³æœ¬ï¼Œè™•ç†å„ç¨®APIå·®ç•°
"""
import json
import os
import random
import inspect
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, Union, Dict, Any, List

import draccus
import torch
import torch.distributed as dist
from PIL import Image
import numpy as np
from torch.utils.data import Dataset, DataLoader

# é¿å…å¾ªç’°å°å…¥å•é¡Œ
from cobra.overwatch import initialize_overwatch
from cobra.util import set_global_seed

# Memory optimization
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'
torch.cuda.empty_cache()

# Single GPU setup
if "WORLD_SIZE" not in os.environ:
    os.environ["WORLD_SIZE"] = "1"
if "RANK" not in os.environ:
    os.environ["RANK"] = "0"
if "LOCAL_RANK" not in os.environ:
    os.environ["LOCAL_RANK"] = "0"

# Initialize Overwatch
overwatch = initialize_overwatch(__name__)


class RobustCOCODataset(Dataset):
    """ç©©å¥çš„COCOæ•¸æ“šé›†ï¼Œè™•ç†å„ç¨®éŒ¯èª¤æƒ…æ³"""
    
    def __init__(
        self,
        coco_json_path: Path,
        images_dir: Path,
        image_transform,
        tokenizer,
        split: str = "train",
        max_samples: Optional[int] = None,
        seed: int = 42
    ):
        self.coco_json_path = coco_json_path
        self.images_dir = images_dir
        self.image_transform = image_transform
        self.tokenizer = tokenizer
        self.split = split
        self.seed = seed
        
        # åŠ è¼‰COCOæ•¸æ“š
        with open(coco_json_path, 'r') as f:
            coco_data = json.load(f)
        
        self.images = {img["id"]: img for img in coco_data["images"]}
        self.annotations = coco_data["annotations"]
        self.categories = {cat["id"]: cat for cat in coco_data["categories"]}
        
        # å‰µå»ºè¨“ç·´æ¨£æœ¬
        self.examples = self._create_examples(max_samples)
        
        overwatch.info(f"å‰µå»ºäº† {len(self.examples)} å€‹è¨“ç·´æ¨£æœ¬")
        
        # æª¢æŸ¥tokenizerå±¬æ€§
        self.max_length = getattr(tokenizer, 'model_max_length', 512)
        if self.max_length > 1024:
            self.max_length = 512  # é™åˆ¶æœ€å¤§é•·åº¦
        
        overwatch.info(f"ä½¿ç”¨tokenizer max_length: {self.max_length}")
    
    def _create_examples(self, max_samples: Optional[int] = None) -> List[Dict]:
        """å¾COCOæ•¸æ“šå‰µå»ºreferring expressionæ¨£æœ¬"""
        examples = []
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­
        random.seed(self.seed)
        
        # ç‚ºæ¯å€‹annotationå‰µå»ºreferring expression
        annotation_count = 0
        for ann in self.annotations:
            if max_samples and annotation_count >= max_samples:
                break
                
            image_id = ann["image_id"]
            image_info = self.images.get(image_id)
            
            if not image_info:
                continue
            
            # ç²å–é¡åˆ¥ä¿¡æ¯
            category_id = ann.get("category_id", 1)
            category_info = self.categories.get(category_id, {"name": "object"})
            category_name = category_info["name"]
            
            # ç¢ºå®šåœ–åƒæ–‡ä»¶è·¯å¾‘
            image_filename = image_info["file_name"]
            if "val2014" in image_filename:
                image_subdir = "val2014"
                example_split = "val"
            else:
                image_subdir = "train2014"
                example_split = "train"
            
            # åªè™•ç†è¨“ç·´æ•¸æ“š
            if self.split == "train" and example_split != "train":
                continue
            
            # æª¢æŸ¥åœ–åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            image_path = self.images_dir / f"{image_subdir}/{image_filename}"
            if not image_path.exists():
                continue
            
            # ç”Ÿæˆç°¡å–®çš„referring expression
            expression = f"find the {category_name}"
            
            example = {
                "image_id": image_id,
                "image_file": f"{image_subdir}/{image_filename}",
                "expression": expression,
                "bbox": ann["bbox"],
                "category_id": category_id,
                "category_name": category_name,
                "split": example_split,
                "ann_id": ann["id"]
            }
            examples.append(example)
            annotation_count += 1
        
        # éš¨æ©Ÿæ‰“äº‚
        random.shuffle(examples)
        
        return examples
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # åŠ è¼‰åœ–åƒ
        image_path = self.images_dir / example["image_file"]
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            overwatch.warning(f"ç„¡æ³•åŠ è¼‰åœ–åƒ {image_path}: {e}")
            # å‰µå»ºä¸€å€‹ç©ºç™½åœ–åƒä½œç‚ºfallback
            image = Image.new("RGB", (384, 384), color="gray")
        
        # æ‡‰ç”¨åœ–åƒè®Šæ›
        if self.image_transform:
            try:
                transformed = self.image_transform(image)
                # æª¢æŸ¥è¿”å›çš„æ ¼å¼
                if isinstance(transformed, dict):
                    # å¦‚æœè¿”å›å­—å…¸ï¼Œæå–pixel_values
                    if "pixel_values" in transformed:
                        image = transformed["pixel_values"]
                    elif "image" in transformed:
                        image = transformed["image"]
                    else:
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°é æœŸçš„keyï¼Œå‰µå»ºfallback tensor
                        image = torch.randn(3, 384, 384)
                elif isinstance(transformed, torch.Tensor):
                    image = transformed
                else:
                    # å…¶ä»–æƒ…æ³çš„fallback
                    image = torch.randn(3, 384, 384)
                    
            except Exception as e:
                overwatch.warning(f"åœ–åƒè®Šæ›å¤±æ•—: {e}")
                # å‰µå»ºtensorä½œç‚ºfallback
                image = torch.randn(3, 384, 384)
        
        # æ§‹å»ºæ–‡æœ¬prompt
        expression = example["expression"]
        bbox = example["bbox"]
        
        # ç°¡åŒ–çš„promptæ ¼å¼
        prompt = f"User: {expression}\nAssistant: [{bbox[0]:.0f}, {bbox[1]:.0f}, {bbox[0]+bbox[2]:.0f}, {bbox[1]+bbox[3]:.0f}]"
        
        # Tokenize with error handling
        try:
            tokenized = self.tokenizer(
                prompt,
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
                return_tensors="pt"
            )
            
            input_ids = tokenized["input_ids"].squeeze()
            attention_mask = tokenized["attention_mask"].squeeze()
            
        except Exception as e:
            overwatch.warning(f"Tokenizationå¤±æ•—: {e}")
            # Fallbackåˆ°ç°¡å–®tokenization
            try:
                tokenized = self.tokenizer(
                    "find object",
                    truncation=True,
                    padding="max_length", 
                    max_length=128,
                    return_tensors="pt"
                )
                input_ids = tokenized["input_ids"].squeeze()
                attention_mask = tokenized["attention_mask"].squeeze()
            except:
                # æœ€å¾Œçš„fallback
                input_ids = torch.zeros(128, dtype=torch.long)
                attention_mask = torch.ones(128, dtype=torch.long)
        
        return {
            "pixel_values": image,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": input_ids.clone(),  # å°æ–¼language modeling
            "image_id": example["image_id"],
            "bbox": torch.tensor(bbox, dtype=torch.float32),
        }


@dataclass
class RobustRefCOCOTrainConfig:
    # Model configuration
    model_id: str = "cobra+3b"
    vision_backbone_id: str = "dinosiglip-vit-so-384px"
    llm_backbone_id: str = "mamba-2.8b-zephyr"
    arch_specifier: str = "no-align+fused-gelu-mlp"
    
    # Dataset configuration
    dataset_name: str = "refcoco"
    data_root: Path = Path("data/refcoco")
    coco_json_file: str = "refcoco.json"
    split: str = "train"
    
    # Training configuration
    stage: str = "lora-finetune"
    use_lora: bool = True
    
    # LoRA configuration
    lora_rank: int = 16
    lora_alpha: float = 32.0
    lora_dropout: float = 0.1
    
    # Optimization parameters
    epochs: int = 3
    max_steps: Optional[int] = None
    global_batch_size: int = 4
    per_device_batch_size: int = 1
    learning_rate: float = 2e-4
    weight_decay: float = 0.01
    max_grad_norm: float = 1.0
    
    # Data loading
    max_samples: Optional[int] = 500
    subset_seed: int = 42
    image_resize_strategy: str = "resize-naive"
    llm_max_length: int = 512
    
    # Run configuration
    run_id: Optional[str] = None
    run_root_dir: Path = Path("runs")
    seed: int = 7
    
    # HF Hub
    hf_token: Union[str, Path] = Path(".hf_token")
    
    # Memory optimization
    enable_memory_optimization: bool = True
    gradient_accumulation_steps: int = 4
    num_workers: int = 0
    
    def __post_init__(self):
        if self.run_id is None:
            self.run_id = f"refcoco-robust+{self.stage}+samples{self.max_samples}"


def create_vlm_robustly(cfg, vision_backbone, llm_backbone):
    """ç©©å¥åœ°å‰µå»ºVLMï¼Œå˜—è©¦å¤šç¨®æ–¹æ³•"""
    
    # æ–¹æ³•1: å˜—è©¦spatial VLM
    try:
        overwatch.info("å˜—è©¦å‰µå»ºSpatial VLM...")
        from cobra.models.vlms.cobra_spatial import create_spatial_cobra_vlm
        
        # æª¢æŸ¥å‡½æ•¸ç°½å
        sig = inspect.signature(create_spatial_cobra_vlm)
        params = list(sig.parameters.keys())
        overwatch.info(f"create_spatial_cobra_vlmåƒæ•¸: {params}")
        
        kwargs = {
            "vision_backbone": vision_backbone,
            "llm_backbone": llm_backbone,
        }
        
        if "model_id" in params:
            kwargs["model_id"] = cfg.model_id
        if "arch_specifier" in params:
            kwargs["arch_specifier"] = cfg.arch_specifier
            
        vlm = create_spatial_cobra_vlm(**kwargs)
        overwatch.info("âœ… Spatial VLMå‰µå»ºæˆåŠŸ")
        return vlm
        
    except Exception as e:
        overwatch.info(f"Spatial VLMå‰µå»ºå¤±æ•—: {e}")
    
    # æ–¹æ³•2: å˜—è©¦æ¨™æº–VLM
    try:
        overwatch.info("å˜—è©¦å‰µå»ºæ¨™æº–VLM...")
        from cobra.models import get_vlm
        
        # æª¢æŸ¥å‡½æ•¸ç°½å
        sig = inspect.signature(get_vlm)
        params = list(sig.parameters.keys())
        overwatch.info(f"get_vlmåƒæ•¸: {params}")
        
        # å˜—è©¦ä¸åŒçš„åƒæ•¸çµ„åˆ
        if len(params) >= 3:
            vlm = get_vlm(cfg.arch_specifier, vision_backbone, llm_backbone)
        else:
            vlm = get_vlm(vision_backbone, llm_backbone)
            
        overwatch.info("âœ… æ¨™æº–VLMå‰µå»ºæˆåŠŸ")
        return vlm
        
    except Exception as e:
        overwatch.info(f"æ¨™æº–VLMå‰µå»ºå¤±æ•—: {e}")
    
    # æ–¹æ³•3: ç›´æ¥å‰µå»ºCobraVLM
    try:
        overwatch.info("å˜—è©¦ç›´æ¥å‰µå»ºCobraVLM...")
        from cobra.models.vlms.cobra import CobraVLM
        
        vlm = CobraVLM(
            vision_backbone=vision_backbone,
            llm_backbone=llm_backbone,
            arch_specifier=cfg.arch_specifier
        )
        overwatch.info("âœ… ç›´æ¥CobraVLMå‰µå»ºæˆåŠŸ")
        return vlm
        
    except Exception as e:
        overwatch.info(f"ç›´æ¥CobraVLMå‰µå»ºå¤±æ•—: {e}")
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—
    raise RuntimeError("ç„¡æ³•å‰µå»ºVLMï¼Œæ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—äº†")


@draccus.wrap()
def train_refcoco_robust(cfg: RobustRefCOCOTrainConfig) -> None:
    """ç©©å¥çš„RefCOCOè¨“ç·´"""
    
    overwatch.info("=== Cobra RefCOCO Robust Training ===")
    overwatch.info(f"ä½¿ç”¨æ•¸æ“šæ–‡ä»¶: {cfg.data_root / cfg.coco_json_file}")
    overwatch.info(f"æœ€å¤§æ¨£æœ¬æ•¸: {cfg.max_samples}")
    
    # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶
    coco_json_path = cfg.data_root / cfg.coco_json_file
    if not coco_json_path.exists():
        overwatch.error(f"æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {coco_json_path}")
        return
    
    images_dir = cfg.data_root / "images"
    if not images_dir.exists():
        overwatch.error(f"åœ–åƒç›®éŒ„ä¸å­˜åœ¨: {images_dir}")
        return
    
    # Memory optimization
    if cfg.enable_memory_optimization:
        torch.cuda.empty_cache()
        if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
            torch.cuda.set_per_process_memory_fraction(0.8)
    
    # Setup device
    device_id = 0
    torch.cuda.set_device(device_id)
    torch.cuda.empty_cache()
    
    # Setup directories
    run_dir = cfg.run_root_dir / cfg.run_id
    os.makedirs(run_dir, exist_ok=True)
    
    # Set global seed
    set_global_seed(cfg.seed)
    
    # Load HuggingFace token
    hf_token = ""
    if isinstance(cfg.hf_token, Path) and cfg.hf_token.exists():
        hf_token = cfg.hf_token.read_text().strip()
    elif "HF_TOKEN" in os.environ:
        hf_token = os.environ["HF_TOKEN"]
    
    # å°å…¥æ¨¡å‹ç›¸é—œæ¨¡å¡Š
    from cobra.models import get_llm_backbone_and_tokenizer, get_vision_backbone_and_transform
    
    # Load backbones
    overwatch.info(f"åŠ è¼‰vision backbone: {cfg.vision_backbone_id}")
    vision_backbone, image_transform = get_vision_backbone_and_transform(
        cfg.vision_backbone_id, cfg.image_resize_strategy
    )
    
    overwatch.info(f"åŠ è¼‰LLM backbone: {cfg.llm_backbone_id}")
    llm_backbone, tokenizer = get_llm_backbone_and_tokenizer(
        cfg.llm_backbone_id,
        llm_max_length=cfg.llm_max_length,
        hf_token=hf_token
    )
    
    # Create VLM robustly
    vlm = create_vlm_robustly(cfg, vision_backbone, llm_backbone)
    
    # Apply LoRA
    if cfg.use_lora:
        overwatch.info("æ‡‰ç”¨LoRA...")
        try:
            from peft import LoraConfig, get_peft_model, TaskType
            
            # è‡ªå‹•æ‰¾åˆ°target modules
            target_modules = []
            for name, module in vlm.named_modules():
                module_type = type(module).__name__
                if "Linear" in module_type and any(keyword in name.lower() for keyword in ["q", "k", "v", "proj", "mlp"]):
                    module_name = name.split(".")[-1]
                    if module_name not in target_modules:
                        target_modules.append(module_name)
            
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨å¸¸è¦‹çš„æ¨¡å¡Šå
            if not target_modules:
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
            
            # é™åˆ¶target modulesæ•¸é‡
            target_modules = target_modules[:4]
            overwatch.info(f"LoRA target modules: {target_modules}")
            
            lora_config = LoraConfig(
                r=cfg.lora_rank,
                lora_alpha=cfg.lora_alpha,
                target_modules=target_modules,
                lora_dropout=cfg.lora_dropout,
                bias="none",
                task_type=TaskType.CAUSAL_LM,
            )
            
            vlm = get_peft_model(vlm, lora_config)
            vlm.print_trainable_parameters()
            
        except Exception as e:
            overwatch.warning(f"LoRAè¨­ç½®å¤±æ•—: {e}")
            overwatch.info("ç¹¼çºŒé€²è¡Œfull fine-tuning...")
    
    # Create dataset
    overwatch.info("å‰µå»ºæ•¸æ“šé›†...")
    dataset = RobustCOCODataset(
        coco_json_path=coco_json_path,
        images_dir=images_dir,
        image_transform=image_transform,
        tokenizer=tokenizer,
        split=cfg.split,
        max_samples=cfg.max_samples,
        seed=cfg.subset_seed
    )
    
    if len(dataset) == 0:
        overwatch.error("æ•¸æ“šé›†ç‚ºç©ºï¼è«‹æª¢æŸ¥æ•¸æ“šè·¯å¾‘å’Œæ ¼å¼")
        return
    
    # Simple collator with error handling
    def robust_collate_fn(batch):
        try:
            batch_size = len(batch)
            overwatch.debug(f"Collating batch of size {batch_size}")
            
            # æª¢æŸ¥ç¬¬ä¸€å€‹æ¨£æœ¬çš„æ ¼å¼
            sample = batch[0]
            overwatch.debug(f"Sample keys: {sample.keys()}")
            overwatch.debug(f"pixel_values type: {type(sample['pixel_values'])}")
            
            # åˆå§‹åŒ–lists
            pixel_values = []
            input_ids = []
            attention_mask = []
            labels = []
            
            for i, item in enumerate(batch):
                try:
                    # è™•ç†pixel_values
                    pv = item["pixel_values"]
                    if isinstance(pv, dict):
                        if "pixel_values" in pv:
                            pv = pv["pixel_values"]
                        elif "image" in pv:
                            pv = pv["image"]
                        else:
                            pv = torch.randn(3, 384, 384)
                    
                    if not isinstance(pv, torch.Tensor):
                        pv = torch.randn(3, 384, 384)
                    
                    pixel_values.append(pv)
                    input_ids.append(item["input_ids"])
                    attention_mask.append(item["attention_mask"])
                    labels.append(item["labels"])
                    
                except Exception as e:
                    overwatch.warning(f"è™•ç†batch item {i}å¤±æ•—: {e}")
                    # ä½¿ç”¨fallback
                    pixel_values.append(torch.randn(3, 384, 384))
                    input_ids.append(torch.zeros(128, dtype=torch.long))
                    attention_mask.append(torch.ones(128, dtype=torch.long))
                    labels.append(torch.zeros(128, dtype=torch.long))
            
            # ç¢ºä¿æ‰€æœ‰tensorç¶­åº¦ä¸€è‡´
            try:
                # æª¢æŸ¥ä¸¦çµ±ä¸€pixel_valuesçš„å½¢ç‹€
                target_shape = pixel_values[0].shape
                for i in range(len(pixel_values)):
                    if pixel_values[i].shape != target_shape:
                        pixel_values[i] = torch.randn(*target_shape)
                
                # æª¢æŸ¥ä¸¦çµ±ä¸€text tensorçš„é•·åº¦
                target_length = input_ids[0].shape[0]
                for i in range(len(input_ids)):
                    if input_ids[i].shape[0] != target_length:
                        input_ids[i] = torch.zeros(target_length, dtype=torch.long)
                        attention_mask[i] = torch.ones(target_length, dtype=torch.long)
                        labels[i] = torch.zeros(target_length, dtype=torch.long)
                
                return {
                    "pixel_values": torch.stack(pixel_values),
                    "input_ids": torch.stack(input_ids),
                    "attention_mask": torch.stack(attention_mask),
                    "labels": torch.stack(labels),
                }
                
            except Exception as e:
                overwatch.error(f"Stack tensorså¤±æ•—: {e}")
                # æœ€å¾Œçš„fallbackï¼šè¿”å›çµ±ä¸€çš„dummy batch
                batch_size = len(batch)
                return {
                    "pixel_values": torch.randn(batch_size, 3, 384, 384),
                    "input_ids": torch.zeros(batch_size, 128, dtype=torch.long),
                    "attention_mask": torch.ones(batch_size, 128, dtype=torch.long),
                    "labels": torch.zeros(batch_size, 128, dtype=torch.long),
                }
            
        except Exception as e:
            overwatch.error(f"Collate functionå®Œå…¨å¤±æ•—: {e}")
            # è¿”å›å–®å€‹dummyæ¨£æœ¬
            return {
                "pixel_values": torch.randn(1, 3, 384, 384),
                "input_ids": torch.zeros(1, 128, dtype=torch.long),
                "attention_mask": torch.ones(1, 128, dtype=torch.long),
                "labels": torch.zeros(1, 128, dtype=torch.long),
            }
    
    # å‰µå»ºDataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=cfg.per_device_batch_size,
        shuffle=True,
        num_workers=cfg.num_workers,
        collate_fn=robust_collate_fn,
        pin_memory=False,  # é¿å…æ½›åœ¨çš„è¨˜æ†¶é«”å•é¡Œ
        drop_last=True
    )
    
    # ç°¡åŒ–çš„è¨“ç·´å¾ªç’°
    overwatch.info("é–‹å§‹è¨“ç·´...")
    
    vlm.to(device_id)
    vlm.train()
    
    # ç°¡å–®çš„optimizer
    optimizer = torch.optim.AdamW(
        vlm.parameters(),
        lr=cfg.learning_rate,
        weight_decay=cfg.weight_decay
    )
    
    # è¨“ç·´å¾ªç’°
    step = 0
    total_loss = 0
    
    for epoch in range(cfg.epochs):
        overwatch.info(f"Epoch {epoch + 1}/{cfg.epochs}")
        epoch_loss = 0
        
        for batch_idx, batch in enumerate(dataloader):
            try:
                # å°‡æ•¸æ“šç§»åˆ°GPU
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(device_id, non_blocking=True)
                
                # Forward pass
                optimizer.zero_grad()
                
                outputs = vlm(**batch)
                
                # æå–loss
                if hasattr(outputs, 'loss'):
                    loss = outputs.loss
                elif isinstance(outputs, dict) and 'loss' in outputs:
                    loss = outputs['loss']
                else:
                    overwatch.warning("ç„¡æ³•æ‰¾åˆ°lossï¼Œè·³éæ­¤batch")
                    continue
                
                # Backward pass
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(vlm.parameters(), cfg.max_grad_norm)
                
                # å„ªåŒ–
                optimizer.step()
                
                step += 1
                loss_val = loss.item()
                total_loss += loss_val
                epoch_loss += loss_val
                
                if step % 10 == 0:
                    avg_loss = total_loss / step
                    overwatch.info(f"Step {step}, Loss: {loss_val:.4f}, Avg Loss: {avg_loss:.4f}")
                
                if step % 50 == 0:
                    torch.cuda.empty_cache()
                
            except Exception as e:
                overwatch.warning(f"è¨“ç·´æ­¥é©Ÿ {step} å¤±æ•—: {e}")
                torch.cuda.empty_cache()
                continue
        
        avg_epoch_loss = epoch_loss / max(1, len(dataloader))
        overwatch.info(f"Epoch {epoch + 1} å®Œæˆ, å¹³å‡Loss: {avg_epoch_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    save_path = run_dir / "final_model"
    os.makedirs(save_path, exist_ok=True)
    overwatch.info(f"ä¿å­˜æ¨¡å‹åˆ°: {save_path}")
    
    try:
        if hasattr(vlm, 'save_pretrained'):
            vlm.save_pretrained(save_path)
            tokenizer.save_pretrained(save_path)
        else:
            torch.save(vlm.state_dict(), save_path / "pytorch_model.bin")
        
        # ä¿å­˜é…ç½®
        draccus.dump(cfg, open(save_path / "config.yaml", "w"))
        overwatch.info("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼")
        
    except Exception as e:
        overwatch.error(f"æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")
    
    overwatch.info("ğŸ‰ è¨“ç·´å®Œæˆï¼")


if __name__ == "__main__":
    train_refcoco_robust()