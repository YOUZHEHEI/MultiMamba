# scripts/train_refcoco_fixed_v2.py
"""
ä¿®å¾©ç‰ˆRefCOCOè¨“ç·´è…³æœ¬
è§£æ±ºç©ºé–“æ¨ç†ä¸­çš„å¼µé‡å°ºå¯¸ä¸åŒ¹é…å•é¡Œ
"""

import logging
import os
from pathlib import Path
from typing import Optional

import draccus
import torch
#from draccus import dataclass
from dataclasses import dataclass
from cobra.conf import ModelConfig
from cobra.models import load_vlm
from cobra.models.vlms.cobra_spatial_fixed import CobraSpatialFixed
from cobra.training import Metrics
from cobra.training.strategies import LoadStrategy
from cobra.preprocessing.datasets.refcoco_dataset import RefCOCODataset
from cobra.preprocessing.multimodal_dataloaders import MultimodalDataLoader


@dataclass
class FixedRefCOCOTrainConfig:
    """ä¿®å¾©ç‰ˆRefCOCOè¨“ç·´é…ç½®"""
    
    # Model & Training Configuration
    model: ModelConfig = ModelConfig()
    refcoco_data_dir: Path = Path("./data/refcoco")
    max_samples: Optional[int] = None
    num_epochs: int = 2
    run_id: str = "refcoco-spatial-fixed"
    use_real_refcoco_data: bool = True
    
    # Spatial Reasoning Configuration
    enable_spatial_reasoning: bool = True
    spatial_module_type: str = "adaptive"
    debug_tensor_shapes: bool = True
    
    # Training Parameters
    batch_size: int = 2
    gradient_accumulation_steps: int = 8
    learning_rate: float = 2e-4
    weight_decay: float = 0.01
    max_grad_norm: float = 1.0
    warmup_ratio: float = 0.05
    
    # GPU Memory Optimization
    use_gradient_checkpointing: bool = True
    mixed_precision: bool = True


def setup_logging():
    """è¨­ç½®æ—¥èªŒ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%m/%d [%H:%M:%S]'
    )
    return logging.getLogger(__name__)


def create_fixed_refcoco_dataset(cfg: FixedRefCOCOTrainConfig, split: str = "train"):
    """å‰µå»ºä¿®å¾©ç‰ˆRefCOCOæ•¸æ“šé›†"""
    
    logger = logging.getLogger(__name__)
    
    # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶
    if cfg.use_real_refcoco_data:
        annotations_file = cfg.refcoco_data_dir / "refcoco.json"
        if not annotations_file.exists():
            logger.warning(f"çœŸå¯¦æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {annotations_file}")
            cfg.use_real_refcoco_data = False
    
    if not cfg.use_real_refcoco_data:
        # å‰µå»ºåˆæˆæ•¸æ“š
        logger.info("å‰µå»ºåˆæˆRefCOCOæ•¸æ“š...")
        synthetic_data = create_synthetic_refcoco_data(cfg.max_samples or 100)
        return SyntheticRefCOCODataset(synthetic_data)
    
    # ä½¿ç”¨çœŸå¯¦æ•¸æ“š
    logger.info(f"è¼‰å…¥çœŸå¯¦RefCOCOæ•¸æ“š: {annotations_file}")
    dataset = RefCOCODataset(
        annotation_file=annotations_file,
        images_dir=cfg.refcoco_data_dir / "images",
        split=split,
        max_samples=cfg.max_samples
    )
    
    return dataset


def create_synthetic_refcoco_data(num_samples: int):
    """å‰µå»ºåˆæˆRefCOCOæ•¸æ“šç”¨æ–¼æ¸¬è©¦"""
    
    import random
    
    synthetic_data = []
    expressions = [
        "the red car on the left",
        "a person in blue shirt",
        "the white building in the center",
        "a dog running in the park",
        "the yellow flower in the garden"
    ]
    
    for i in range(num_samples):
        # å‰µå»ºéš¨æ©Ÿé‚Šç•Œæ¡†
        x = random.uniform(0, 400)
        y = random.uniform(0, 300)
        w = random.uniform(50, 200)
        h = random.uniform(50, 150)
        
        example = {
            "image_id": f"synthetic_{i}",
            "image_path": "synthetic_image.jpg",  # å°‡ä½¿ç”¨éš¨æ©Ÿåœ–åƒ
            "expression": random.choice(expressions),
            "bbox": [x, y, w, h],
            "image_width": 640,
            "image_height": 480
        }
        synthetic_data.append(example)
    
    return synthetic_data


class SyntheticRefCOCODataset:
    """åˆæˆRefCOCOæ•¸æ“šé›†ç”¨æ–¼æ¸¬è©¦"""
    
    def __init__(self, synthetic_data):
        self.examples = synthetic_data
        
        # å‰µå»ºåˆæˆåœ–åƒè®Šæ›
        from torchvision import transforms
        self.image_transform = transforms.Compose([
            transforms.Resize((384, 384)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # å‰µå»ºéš¨æ©Ÿåœ–åƒ
        import torch
        from PIL import Image
        import numpy as np
        
        # ç”Ÿæˆéš¨æ©ŸRGBåœ–åƒ
        random_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        image = Image.fromarray(random_image)
        pixel_values = self.image_transform(image)
        
        # å‰µå»ºè¼¸å…¥æ–‡æœ¬
        prompt = f"Please locate: {example['expression']}"
        
        return {
            "pixel_values": pixel_values,
            "text": prompt,
            "bbox": torch.tensor(example["bbox"], dtype=torch.float32),
            "image_size": torch.tensor([example["image_width"], example["image_height"]], dtype=torch.float32)
        }
    
    def get_modality_lengths(self):
        """è¿”å›æ¨¡æ…‹é•·åº¦ä¿¡æ¯"""
        return [(True, 20) for _ in range(len(self.examples))]  # æ‰€æœ‰æ¨£æœ¬éƒ½æ˜¯å¤šæ¨¡æ…‹


def create_fixed_spatial_model(cfg: FixedRefCOCOTrainConfig):
    """å‰µå»ºä¿®å¾©ç‰ˆç©ºé–“æ¨ç†æ¨¡å‹"""
    
    logger = logging.getLogger(__name__)
    logger.info("è¼‰å…¥æ¨¡å‹çµ„ä»¶...")
    
    # åŸºæ–¼é…ç½®å‰µå»ºæ¨¡å‹
    if cfg.model.type == "cobra-refcoco-lora+3b":
        model_config = ModelConfig(
            model_id="cobra-spatial-fixed+3b",
            arch_specifier="no-align+fused-gelu-mlp",
            vision_backbone_id="dinosiglip-vit-so-384px",
            llm_backbone_id="mamba-2.8b-zephyr",
            image_resize_strategy="resize-naive",
            llm_max_length=2048,
        )
    else:
        model_config = cfg.model
    
    # ä½¿ç”¨ä¿®å¾©ç‰ˆçš„ç©ºé–“æ¨ç†æ¨¡å‹
    vlm = CobraSpatialFixed.from_pretrained(
        model_config.model_id,
        enable_spatial_reasoning=cfg.enable_spatial_reasoning,
        spatial_module_type=cfg.spatial_module_type,
        debug_tensor_shapes=cfg.debug_tensor_shapes,
    )
    
    # æª¢æŸ¥æ¨¡å‹åƒæ•¸
    total_params = sum(p.numel() for p in vlm.parameters())
    trainable_params = sum(p.numel() for p in vlm.parameters() if p.requires_grad)
    
    logger.info(f"ç¸½åƒæ•¸æ•¸é‡: {total_params:,}")
    logger.info(f"å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
    
    return vlm


def train_with_tensor_shape_debugging(vlm, dataloader, cfg: FixedRefCOCOTrainConfig):
    """å¸¶å¼µé‡å½¢ç‹€èª¿è©¦çš„è¨“ç·´å‡½æ•¸"""
    
    logger = logging.getLogger(__name__)
    
    # è¨­ç½®å„ªåŒ–å™¨
    optimizer = torch.optim.AdamW(
        vlm.parameters(),
        lr=cfg.learning_rate,
        weight_decay=cfg.weight_decay
    )
    
    vlm.train()
    
    for epoch in range(cfg.num_epochs):
        logger.info(f"é–‹å§‹ Epoch {epoch + 1}/{cfg.num_epochs}")
        
        epoch_loss = 0.0
        num_batches = 0
        
        for batch_idx, batch in enumerate(dataloader):
            try:
                # èª¿è©¦ï¼šæ‰“å°æ‰¹æ¬¡ä¿¡æ¯
                if cfg.debug_tensor_shapes:
                    logger.info(f"æ‰¹æ¬¡ {batch_idx}: ")
                    for key, value in batch.items():
                        if isinstance(value, torch.Tensor):
                            logger.info(f"  {key}: {value.shape}")
                        else:
                            logger.info(f"  {key}: {type(value)}")
                
                # å‰å‘å‚³æ’­
                outputs = vlm(**batch)
                loss = outputs.loss
                
                # åå‘å‚³æ’­
                loss = loss / cfg.gradient_accumulation_steps
                loss.backward()
                
                if (batch_idx + 1) % cfg.gradient_accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(vlm.parameters(), cfg.max_grad_norm)
                    
                    # å„ªåŒ–å™¨æ­¥é©Ÿ
                    optimizer.step()
                    optimizer.zero_grad()
                
                epoch_loss += loss.item() * cfg.gradient_accumulation_steps
                num_batches += 1
                
                if batch_idx % 10 == 0:
                    logger.info(f"  æ‰¹æ¬¡ {batch_idx}, æå¤±: {loss.item():.4f}")
                
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {batch_idx} è™•ç†å¤±æ•—: {e}")
                if cfg.debug_tensor_shapes:
                    import traceback
                    traceback.print_exc()
                continue
        
        avg_loss = epoch_loss / max(num_batches, 1)
        logger.info(f"Epoch {epoch + 1} å¹³å‡æå¤±: {avg_loss:.4f}")


@draccus.wrap()
def train_fixed_refcoco(cfg: FixedRefCOCOTrainConfig):
    """ä¿®å¾©ç‰ˆRefCOCOè¨“ç·´ä¸»å‡½æ•¸"""
    
    logger = setup_logging()
    logger.info("ğŸš€ é–‹å§‹ä¿®å¾©ç‰ˆRefCOCOè¨“ç·´")
    logger.info(f"æ¨¡å‹: {cfg.model.type}")
    logger.info(f"æ¨£æœ¬æ•¸: {cfg.max_samples}")
    logger.info(f"çœŸå¯¦æ•¸æ“š: {cfg.use_real_refcoco_data}")
    logger.info(f"ç©ºé–“æ¨ç†: {cfg.enable_spatial_reasoning}")
    logger.info(f"èª¿è©¦æ¨¡å¼: {cfg.debug_tensor_shapes}")
    
    try:
        # å‰µå»ºæ•¸æ“šé›†
        logger.info("å‰µå»ºä¿®å¾©ç‰ˆRefCOCOæ•¸æ“šé›†...")
        dataset = create_fixed_refcoco_dataset(cfg)
        logger.info(f"æˆåŠŸè¼‰å…¥RefCOCOæ•¸æ“š: {len(dataset)} æ¢ç›®")
        
        # å‰µå»ºæ¨¡å‹
        vlm = create_fixed_spatial_model(cfg)
        
        # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
        dataloader = MultimodalDataLoader(
            [dataset],
            collate_fn=vlm.get_collate_fn(),
            batch_size=cfg.batch_size,
            shuffle=True,
            num_workers=0  # å–®ç·šç¨‹é¿å…å•é¡Œ
        )
        
        # GPUå…§å­˜æª¢æŸ¥
        if torch.cuda.is_available():
            device = torch.device("cuda")
            vlm = vlm.to(device)
            
            allocated = torch.cuda.memory_allocated() / 1e9
            cached = torch.cuda.memory_reserved() / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            logger.info(f"GPUå…§å­˜ - å·²åˆ†é…: {allocated:.1f}GB, å·²ç·©å­˜: {cached:.1f}GB, ç¸½è¨ˆ: {total:.1f}GB")
        
        # é–‹å§‹è¨“ç·´
        logger.info("ğŸ¯ é–‹å§‹ä¿®å¾©ç‰ˆè¨“ç·´...")
        train_with_tensor_shape_debugging(vlm, dataloader, cfg)
        
        logger.info("âœ… è¨“ç·´å®Œæˆ!")
        
        # ä¿å­˜æ¨¡å‹
        save_path = Path(f"./runs/{cfg.run_id}")
        save_path.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': vlm.state_dict(),
            'config': cfg,
            'epoch': cfg.num_epochs
        }, save_path / "spatial_fixed_model.pt")
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        
    except Exception as e:
        logger.error(f"è¨“ç·´éç¨‹éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    train_fixed_refcoco()