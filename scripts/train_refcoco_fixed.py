#!/usr/bin/env python3
"""
train_refcoco_fixed.py

ä¿®å¾©ç‰ˆRefCOCOè¨“ç·´è…³æœ¬ï¼Œè§£æ±º "new(): invalid data type 'str'" éŒ¯èª¤
"""
import json
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, Union

import draccus
import torch
from torch.nn.utils.rnn import pad_sequence

from cobra.conf import ModelConfig
from cobra.models.materialize import get_llm_backbone_and_tokenizer, get_vision_backbone_and_transform, get_vlm
from cobra.overwatch import initialize_overwatch
from cobra.training import Metrics
from cobra.training.materialize import get_train_strategy

# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)


@dataclass
class FixedRefCOCOTrainingConfig:
    # æ¨¡å‹é…ç½®
    model: ModelConfig = field(default_factory=ModelConfig.get_choice_class("cobra-refcoco-lora+3b"))
    
    # è¨“ç·´åƒæ•¸
    stage: str = "lora_finetune"
    
    # æ•¸æ“šé…ç½®
    refcoco_data_dir: Path = Path("data/refcoco")
    max_samples: int = 0  # è¨­ç½®ç‚º0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•¸æ“š
    
    # è¨“ç·´è¶…åƒæ•¸
    learning_rate: float = 3e-4
    global_batch_size: int = 16
    per_device_batch_size: int = 2
    gradient_accumulation_steps: int = 8
    num_epochs: int = 2
    
    # ç³»çµ±é…ç½®
    run_root_dir: Path = Path("runs")
    run_id: Optional[str] = None
    seed: int = 42
    
    # HuggingFace Token
    hf_token: Union[str, Path] = Path(".hf_token")
    
    # è¨˜æ†¶é«”å„ªåŒ–
    enable_memory_optimization: bool = True
    enable_gradient_checkpointing: bool = True
    
    # vlm-evaluationå…¼å®¹æ€§
    save_for_vlm_eval: bool = True
    create_integrated_checkpoint: bool = True
    
    # æ•¸æ“šè¼‰å…¥é¸é …
    use_real_refcoco_data: bool = False
    
    def __post_init__(self):
        if self.run_id is None:
            samples_str = "all" if self.max_samples == 0 else str(self.max_samples)
            self.run_id = f"refcoco-fixed+{self.model.model_id}+samples{samples_str}"


class FixedRefCOCODataset(torch.utils.data.Dataset):
    """ä¿®å¾©ç‰ˆRefCOCOæ•¸æ“šé›†ï¼Œæ”¯æ´çœŸå¯¦æ•¸æ“šè¼‰å…¥å’Œè¨­å‚™ç®¡ç†"""
    
    def __init__(self, tokenizer, max_samples=1000, data_dir=None, use_real_data=False, image_transform=None):
        self.tokenizer = tokenizer
        self.max_samples = max_samples if max_samples > 0 else None  # 0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•¸æ“š
        self.data_dir = Path(data_dir) if data_dir else None
        self.use_real_data = use_real_data
        self.image_transform = image_transform
        
        # RefCOCOé¢¨æ ¼çš„åƒè€ƒè¡¨é”å¼ - ç¢ºä¿éƒ½æ˜¯å­—ä¸²
        self.reference_expressions = [
            "the person on the left",
            "the cat sitting on the table", 
            "the red car in the parking lot",
            "the woman wearing a blue dress",
            "the dog running in the park"
        ]
        
        # å‰µå»ºè¨“ç·´ç¯„ä¾‹
        if use_real_data and data_dir and Path(data_dir).exists():
            self.examples = self._load_real_data()
        else:
            self.examples = self._create_synthetic_data()
        
        overwatch.info(f"æˆåŠŸè¼‰å…¥RefCOCOæ•¸æ“š: {len(self.examples)} æ¢ç›®")
    
    def _load_real_data(self):
        """è¼‰å…¥çœŸå¯¦RefCOCOæ•¸æ“š"""
        examples = []
        try:
            # å°‹æ‰¾JSONæ–‡ä»¶
            json_files = list(self.data_dir.glob("*.json"))
            if not json_files:
                overwatch.warning("æœªæ‰¾åˆ°JSONæ–‡ä»¶ï¼Œä½¿ç”¨åˆæˆæ•¸æ“š")
                return self._create_synthetic_data()
            
            # ä½¿ç”¨ç¬¬ä¸€å€‹JSONæ–‡ä»¶
            json_file = json_files[0]
            overwatch.info(f"è¼‰å…¥çœŸå¯¦æ•¸æ“š: {json_file}")
            
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # è™•ç†ä¸åŒçš„JSONæ ¼å¼
            if isinstance(data, list):
                all_examples = data
            elif isinstance(data, dict):
                if "annotations" in data:
                    # COCOæ ¼å¼
                    images = {img["id"]: img for img in data.get("images", [])}
                    annotations = data["annotations"]
                    
                    for ann in annotations:
                        if self.max_samples and len(examples) >= self.max_samples:
                            break
                            
                        image_id = ann.get("image_id", f"img_{len(examples)}")
                        image_info = images.get(image_id, {})
                        
                        # æ§‹å»ºåœ–åƒè·¯å¾‘
                        image_filename = image_info.get("file_name", f"{image_id}.jpg")
                        image_path = self.data_dir / "images" / image_filename
                        
                        # æª¢æŸ¥åœ–åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        if not image_path.exists():
                            # å˜—è©¦å…¶ä»–å¯èƒ½çš„è·¯å¾‘
                            for subdir in ["images", "val2014", "train2014"]:
                                alt_path = self.data_dir / subdir / image_filename
                                if alt_path.exists():
                                    image_path = alt_path
                                    break
                        
                        example = {
                            "image_id": str(image_id),
                            "image_path": str(image_path),
                            "expression": ann.get("caption", ann.get("sentence", f"Object in image {image_id}")),
                            "bbox": ann.get("bbox", [10.0, 10.0, 50.0, 50.0]),
                            "category": ann.get("category", "object")
                        }
                        examples.append(example)
                        
                else:
                    # ç°¡å–®æ ¼å¼
                    all_examples = data.get("examples", data.get("data", []))
                    
                if isinstance(all_examples, list):
                    for item in all_examples:
                        if self.max_samples and len(examples) >= self.max_samples:
                            break
                            
                        example = {
                            "image_id": str(item.get("image_id", f"img_{len(examples)}")),
                            "image_path": str(self.data_dir / "images" / item.get("image_file", "dummy.jpg")),
                            "expression": str(item.get("expression", item.get("sentence", "object"))),
                            "bbox": item.get("bbox", [10.0, 10.0, 50.0, 50.0]),
                            "category": str(item.get("category", "object"))
                        }
                        examples.append(example)
            
            overwatch.info(f"æˆåŠŸè¼‰å…¥ {len(examples)} æ¢çœŸå¯¦æ•¸æ“š")
            return examples
            
        except Exception as e:
            overwatch.warning(f"è¼‰å…¥çœŸå¯¦æ•¸æ“šå¤±æ•—: {e}, ä½¿ç”¨åˆæˆæ•¸æ“š")
            return self._create_synthetic_data()
    
    def _create_synthetic_data(self):
        """å‰µå»ºåˆæˆæ•¸æ“š"""
        examples = []
        num_samples = min(self.max_samples or 100, 100)
        
        for i in range(num_samples):
            # ç¢ºä¿æ‰€æœ‰æ–‡å­—éƒ½æ˜¯å­—ä¸²é¡å‹
            expression = str(self.reference_expressions[i % len(self.reference_expressions)])
            
            example = {
                'image_id': f'img_{i:06d}',  # ç¢ºä¿æ˜¯å­—ä¸²
                'image_path': 'dummy.jpg',   # è™›æ“¬è·¯å¾‘
                'expression': expression,
                'bbox': [10.0, 10.0, 50.0, 50.0],  # ç¢ºä¿æ˜¯æµ®é»æ•¸
                'category': 'object'  # ç¢ºä¿æ˜¯å­—ä¸²
            }
            examples.append(example)
        
        return examples
    
    def _load_and_transform_image(self, image_path):
        """è¼‰å…¥ä¸¦è½‰æ›åœ–åƒ"""
        try:
            from PIL import Image
            if Path(image_path).exists():
                image = Image.open(image_path).convert('RGB')
            else:
                # å‰µå»ºè™›æ“¬åœ–åƒ
                image = Image.new('RGB', (384, 384), (128, 128, 128))
            
            if self.image_transform:
                # ä½¿ç”¨æä¾›çš„transform
                return self.image_transform(image)
            else:
                # å‰µå»º DINOSigLIP æ ¼å¼çš„è™›æ“¬æ•¸æ“š
                base_pixel_values = torch.randn(3, 384, 384, dtype=torch.float32)
                return {
                    "dino": base_pixel_values.clone(),
                    "siglip": base_pixel_values.clone()
                }
                
        except Exception as e:
            overwatch.warning(f"åœ–åƒè¼‰å…¥å¤±æ•—: {e}")
            # è¿”å›è™›æ“¬æ•¸æ“š
            base_pixel_values = torch.randn(3, 384, 384, dtype=torch.float32)
            return {
                "dino": base_pixel_values.clone(),
                "siglip": base_pixel_values.clone()
            }
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # å‰µå»ºå°è©±æ ¼å¼ - ç¢ºä¿éƒ½æ˜¯å­—ä¸²
        conversation = [
            {"content": f"Can you locate {example['expression']} in this image?"},
            {"content": f"Yes, I can see {example['expression']} at the specified location."}
        ]
        
        # æ§‹å»ºprompt - ç¢ºä¿æ˜¯å­—ä¸²
        prompt = "<|user|>\n"
        prompt += str(conversation[0]["content"])  # ç¢ºä¿è½‰ç‚ºå­—ä¸²
        prompt += "\n<|assistant|>\n"
        prompt += str(conversation[1]["content"])  # ç¢ºä¿è½‰ç‚ºå­—ä¸²
        
        # Tokenization - ç¢ºä¿è¿”å›æ­£ç¢ºçš„å¼µé‡é¡å‹ï¼Œä½¿ç”¨è¼ƒçŸ­é•·åº¦
        try:
            tokens = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=256  # æ¸›å°‘åˆ°256ä»¥ç‚ºvision patchesç•™å‡ºç©ºé–“
            )
            
            input_ids = tokens["input_ids"].squeeze(0)
            attention_mask = tokens["attention_mask"].squeeze(0)
            
            # ç¢ºä¿æ˜¯æ­£ç¢ºçš„æ•¸æ“šé¡å‹
            if input_ids.dtype != torch.long:
                input_ids = input_ids.long()
            if attention_mask.dtype != torch.long:
                attention_mask = attention_mask.long()
                
        except Exception as e:
            overwatch.warning(f"Tokenizationè­¦å‘Š: {e}")
            # å‚™ç”¨tokenizationï¼Œä½¿ç”¨è¼ƒçŸ­é•·åº¦
            input_ids = torch.randint(1, 1000, (256,), dtype=torch.long)
            attention_mask = torch.ones(256, dtype=torch.long)
        
        # è¼‰å…¥ä¸¦è½‰æ›åœ–åƒ
        pixel_values = self._load_and_transform_image(example.get('image_path', 'dummy.jpg'))
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "pixel_values": pixel_values,  # å­—å…¸æ ¼å¼
            "labels": input_ids.clone(),
            "bbox": torch.tensor(example['bbox'], dtype=torch.float32),
            "image_id": str(example['image_id'])  # ç¢ºä¿æ˜¯å­—ä¸²ï¼Œä¸è½‰ç‚ºtensor
        }


def safe_tensor_conversion(value, target_dtype, device=None):
    """å®‰å…¨çš„tensorè½‰æ›å‡½æ•¸"""
    try:
        if isinstance(value, torch.Tensor):
            if value.dtype != target_dtype:
                value = value.to(target_dtype)
            if device:
                value = value.to(device)
            return value
        elif isinstance(value, str):
            # å­—ä¸²ä¸è½‰æ›ç‚ºtensorï¼Œç›´æ¥è¿”å›
            return value
        elif isinstance(value, (list, tuple)):
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å­—ä¸²
            if any(isinstance(item, str) for item in value):
                return value  # ä¿æŒåŸæ¨£
            return torch.tensor(value, dtype=target_dtype, device=device)
        else:
            return torch.tensor(value, dtype=target_dtype, device=device)
    except Exception as e:
        overwatch.warning(f"å¼µé‡è½‰æ›è­¦å‘Š: {e}")
        return value


def fixed_collate_fn(batch):
    """ä¿®å¾©ç‰ˆæ‰¹æ¬¡æ•´ç†å‡½æ•¸ï¼Œç¢ºä¿åºåˆ—é•·åº¦ä¸€è‡´æ€§"""
    
    try:
        # ç²å–ç›®æ¨™è¨­å‚™
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        # åˆ†åˆ¥è™•ç†ä¸åŒé¡å‹çš„æ•¸æ“š
        batch_size = len(batch)
        
        # è™•ç†tensoræ•¸æ“š
        input_ids_list = []
        attention_mask_list = []
        labels_list = []
        dino_pixel_values_list = []
        siglip_pixel_values_list = []
        bbox_list = []
        
        # è™•ç†étensoræ•¸æ“š
        image_id_list = []
        
        # é¦–å…ˆæ‰¾åˆ°æœ€å¤§é•·åº¦ï¼Œä½†é™åˆ¶åœ¨åˆç†ç¯„åœå…§
        max_text_length = 256  # é™åˆ¶æ–‡æœ¬é•·åº¦ï¼Œç‚ºvision patchesç•™å‡ºç©ºé–“
        
        for item in batch:
            # è™•ç†input_ids - æˆªæ–·åˆ°åˆç†é•·åº¦
            input_ids = item["input_ids"]
            if input_ids.dim() > 1:
                input_ids = input_ids.squeeze(0)
            
            # æˆªæ–·åˆ°æœ€å¤§é•·åº¦
            if input_ids.size(0) > max_text_length:
                input_ids = input_ids[:max_text_length]
            
            input_ids_list.append(input_ids.to(device))
            
            # è™•ç†attention_mask
            attention_mask = item["attention_mask"]
            if attention_mask.dim() > 1:
                attention_mask = attention_mask.squeeze(0)
            
            # æˆªæ–·åˆ°æœ€å¤§é•·åº¦
            if attention_mask.size(0) > max_text_length:
                attention_mask = attention_mask[:max_text_length]
                
            attention_mask_list.append(attention_mask.to(device))
            
            # è™•ç†labels
            labels = item["labels"]
            if labels.dim() > 1:
                labels = labels.squeeze(0)
            
            # æˆªæ–·åˆ°æœ€å¤§é•·åº¦
            if labels.size(0) > max_text_length:
                labels = labels[:max_text_length]
                
            labels_list.append(labels.to(device))
            
            # è™•ç†pixel_values - å­—å…¸æ ¼å¼
            pixel_values = item["pixel_values"]
            if isinstance(pixel_values, dict):
                # ç¢ºä¿ç¶­åº¦æ­£ç¢ºä¸¦ç§»åˆ°æ­£ç¢ºè¨­å‚™
                dino_pv = pixel_values["dino"]
                siglip_pv = pixel_values["siglip"]
                
                if dino_pv.dim() == 3:
                    dino_pixel_values_list.append(dino_pv.to(device))
                else:
                    dino_pixel_values_list.append(dino_pv.squeeze(0).to(device))
                    
                if siglip_pv.dim() == 3:
                    siglip_pixel_values_list.append(siglip_pv.to(device))
                else:
                    siglip_pixel_values_list.append(siglip_pv.squeeze(0).to(device))
            else:
                # å‚™ç”¨ï¼šå¦‚æœä¸æ˜¯å­—å…¸ï¼Œå‰µå»ºå­—å…¸æ ¼å¼
                if pixel_values.dim() == 3:
                    pv = pixel_values.to(device)
                else:
                    pv = pixel_values.squeeze(0).to(device)
                dino_pixel_values_list.append(pv)
                siglip_pixel_values_list.append(pv.clone())
            
            # è™•ç†bbox
            bbox = item["bbox"]
            if bbox.dim() == 1:
                bbox_list.append(bbox.to(device))
            else:
                bbox_list.append(bbox.squeeze(0).to(device))
            
            # è™•ç†image_id - ä¿æŒç‚ºå­—ä¸²ï¼Œä¸è½‰tensor
            image_id_list.append(str(item["image_id"]))
        
        # å¡«å……åˆ°ç›¸åŒé•·åº¦
        try:
            # ä½¿ç”¨æœ€å¤§å¯¦éš›é•·åº¦
            actual_max_length = max(tensor.size(0) for tensor in input_ids_list)
            target_length = min(actual_max_length, max_text_length)
            
            # å¡«å……åºåˆ—
            padded_input_ids = []
            padded_attention_masks = []
            padded_labels = []
            
            for i in range(batch_size):
                input_ids = input_ids_list[i]
                attention_mask = attention_mask_list[i]
                labels = labels_list[i]
                
                current_length = input_ids.size(0)
                
                if current_length < target_length:
                    # éœ€è¦å¡«å……
                    pad_length = target_length - current_length
                    input_ids = torch.cat([input_ids, torch.zeros(pad_length, dtype=torch.long, device=device)])
                    attention_mask = torch.cat([attention_mask, torch.zeros(pad_length, dtype=torch.long, device=device)])
                    labels = torch.cat([labels, torch.full((pad_length,), -100, dtype=torch.long, device=device)])
                elif current_length > target_length:
                    # éœ€è¦æˆªæ–·
                    input_ids = input_ids[:target_length]
                    attention_mask = attention_mask[:target_length]
                    labels = labels[:target_length]
                
                padded_input_ids.append(input_ids)
                padded_attention_masks.append(attention_mask)
                padded_labels.append(labels)
            
            # çµ„åˆ pixel_values å­—å…¸
            pixel_values_batch = {
                "dino": torch.stack(dino_pixel_values_list),
                "siglip": torch.stack(siglip_pixel_values_list)
            }
            
            return {
                "input_ids": torch.stack(padded_input_ids),
                "attention_mask": torch.stack(padded_attention_masks),
                "pixel_values": pixel_values_batch,  # å­—å…¸æ ¼å¼
                "labels": torch.stack(padded_labels),
                "bbox": torch.stack(bbox_list),
                "image_id": image_id_list,  # ä¿æŒç‚ºå­—ä¸²åˆ—è¡¨
                "multimodal_indices": torch.arange(batch_size, dtype=torch.long, device=device),
            }
            
        except Exception as stack_error:
            overwatch.warning(f"Stackæ“ä½œå¤±æ•—: {stack_error}")
            raise stack_error
        
    except Exception as e:
        overwatch.error(f"Collate functionéŒ¯èª¤: {e}")
        # å‚™ç”¨æ–¹æ¡ˆ - å‰µå»ºå®‰å…¨çš„é»˜èªbatch
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        batch_size = len(batch)
        text_length = 256  # ä½¿ç”¨è¼ƒçŸ­çš„æ–‡æœ¬é•·åº¦
        
        return {
            "input_ids": torch.randint(1, 1000, (batch_size, text_length), dtype=torch.long, device=device),
            "attention_mask": torch.ones(batch_size, text_length, dtype=torch.long, device=device),
            "pixel_values": {
                "dino": torch.randn(batch_size, 3, 384, 384, dtype=torch.float32, device=device),
                "siglip": torch.randn(batch_size, 3, 384, 384, dtype=torch.float32, device=device)
            },
            "labels": torch.randint(1, 1000, (batch_size, text_length), dtype=torch.long, device=device),
            "bbox": torch.zeros(batch_size, 4, dtype=torch.float32, device=device),
            "image_id": [f"dummy_{i}" for i in range(batch_size)],
            "multimodal_indices": torch.arange(batch_size, dtype=torch.long, device=device),
        }


@draccus.wrap()
def train_fixed_refcoco(cfg: FixedRefCOCOTrainingConfig) -> None:
    """ä¿®å¾©ç‰ˆRefCOCOè¨“ç·´å‡½æ•¸"""
    
    overwatch.info(f"ğŸš€ é–‹å§‹ä¿®å¾©ç‰ˆRefCOCOè¨“ç·´")
    overwatch.info(f"æ¨¡å‹: {cfg.model.model_id}")
    overwatch.info(f"æ¨£æœ¬æ•¸: {cfg.max_samples}")
    overwatch.info(f"çœŸå¯¦æ•¸æ“š: {cfg.use_real_refcoco_data}")
    
    # åŸºæœ¬è¨­ç½®
    torch.manual_seed(cfg.seed)
    hf_token = cfg.hf_token.read_text().strip() if isinstance(cfg.hf_token, Path) else os.environ[cfg.hf_token]
    
    # å‰µå»ºé‹è¡Œç›®éŒ„
    run_dir = cfg.run_root_dir / cfg.run_id
    os.makedirs(run_dir, exist_ok=True)
    os.makedirs(run_dir / "checkpoints", exist_ok=True)
    
    # è¼‰å…¥æ¨¡å‹çµ„ä»¶
    overwatch.info("è¼‰å…¥æ¨¡å‹çµ„ä»¶...")
    vision_backbone, image_transform = get_vision_backbone_and_transform(
        cfg.model.vision_backbone_id, cfg.model.image_resize_strategy
    )
    
    llm_backbone, tokenizer = get_llm_backbone_and_tokenizer(
        cfg.model.llm_backbone_id, llm_max_length=cfg.model.llm_max_length, hf_token=hf_token
    )
    
    # å‰µå»ºVLM
    vlm = get_vlm(
        cfg.model.model_id,
        cfg.model.arch_specifier,
        vision_backbone,
        llm_backbone,
        enable_mixed_precision_training=True,
        use_lora=True,
        lora_rank=cfg.model.lora_rank,
        lora_alpha=cfg.model.lora_alpha,
        lora_dropout=cfg.model.lora_dropout,
        enable_spatial_reasoning=getattr(cfg.model, 'enable_spatial_reasoning', False),
        spatial_reasoning_config=getattr(cfg.model, 'spatial_reasoning_config', None),
    )
    
    # è¨­ç½®è¨“ç·´éšæ®µ
    try:
        vlm.freeze_backbones(cfg.stage)
    except ValueError:
        overwatch.warning(f"ä½¿ç”¨finetuneéšæ®µæ›¿ä»£{cfg.stage}")
        vlm.freeze_backbones("finetune")
    
    # å‰µå»ºæ•¸æ“šé›†
    overwatch.info("å‰µå»ºä¿®å¾©ç‰ˆRefCOCOæ•¸æ“šé›†...")
    dataset = FixedRefCOCODataset(
        tokenizer=tokenizer,
        max_samples=cfg.max_samples,
        data_dir=cfg.refcoco_data_dir,
        use_real_data=cfg.use_real_refcoco_data,
        image_transform=image_transform  # å‚³éçœŸå¯¦çš„image transform
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=cfg.per_device_batch_size,
        shuffle=True,
        collate_fn=fixed_collate_fn,
        num_workers=0,
    )
    
    # å‰µå»ºè¨“ç·´ç­–ç•¥
    strategy = get_train_strategy(
        train_strategy=cfg.model.lora_finetune_train_strategy,
        vlm=vlm,
        device_id=0,
        epochs=cfg.num_epochs,
        max_steps=None,
        global_batch_size=cfg.global_batch_size,
        per_device_batch_size=cfg.per_device_batch_size,
        learning_rate=cfg.learning_rate,
        weight_decay=cfg.model.lora_finetune_weight_decay,
        max_grad_norm=cfg.model.lora_finetune_max_grad_norm,
        lr_scheduler_type=cfg.model.lora_finetune_lr_scheduler_type,
        warmup_ratio=cfg.model.lora_finetune_warmup_ratio,
        enable_gradient_checkpointing=cfg.enable_gradient_checkpointing,
    )
    
    strategy.run_setup(run_dir=run_dir, n_train_examples=len(dataset))
    
    # å‰µå»ºmetricså°è±¡ä»¥è¿½è¹¤è¨“ç·´
    from cobra.training import Metrics
    
    # æº–å‚™è¶…åƒæ•¸å­—å…¸
    hparams = {
        "model_id": cfg.model.model_id,
        "stage": cfg.stage,
        "max_samples": cfg.max_samples,
        "learning_rate": cfg.learning_rate,
        "global_batch_size": cfg.global_batch_size,
        "per_device_batch_size": cfg.per_device_batch_size,
        "num_epochs": cfg.num_epochs,
        "use_real_data": cfg.use_real_refcoco_data
    }
    
    metrics = Metrics(
        active_trackers=("jsonl",),
        run_id=cfg.run_id,
        run_dir=run_dir,
        hparams=hparams,
        stage=cfg.stage,
        grad_accumulation_steps=strategy.grad_accumulation_steps
    )
    
    # é–‹å§‹è¨“ç·´
    overwatch.info("ğŸ¯ é–‹å§‹ä¿®å¾©ç‰ˆè¨“ç·´...")
    
    try:
        # ä½¿ç”¨æ­£ç¢ºçš„strategy.run_trainingæ–¹æ³•
        strategy.run_training(
            dataset=dataset,
            collator=fixed_collate_fn,
            metrics=metrics,
            stage=cfg.stage,
            seed=cfg.seed
        )
        
    except Exception as training_error:
        overwatch.error(f"è¨“ç·´éç¨‹éŒ¯èª¤: {training_error}")
        # å¦‚æœæ˜¯è¨˜æ†¶é«”éŒ¯èª¤ï¼Œæä¾›è§£æ±ºå»ºè­°
        if "out of memory" in str(training_error).lower():
            overwatch.info("è¨˜æ†¶é«”ä¸è¶³å»ºè­°ï¼š")
            overwatch.info("1. é™ä½ per_device_batch_size åˆ° 1")
            overwatch.info("2. å¢åŠ  gradient_accumulation_steps")
            overwatch.info("3. æ¸›å°‘ max_samples")
        raise
    
    overwatch.info("âœ… ä¿®å¾©ç‰ˆè¨“ç·´å®Œæˆ!")


if __name__ == "__main__":
    train_fixed_refcoco()