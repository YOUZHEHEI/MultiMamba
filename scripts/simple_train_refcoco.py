#!/usr/bin/env python3
"""
simple_train_refcoco.py

ç®€åŒ–çš„RefCOCOè®­ç»ƒè„šæœ¬ï¼Œä½¿ç”¨æ ‡å‡†VLMé¿å¼€ç©ºé—´æ¨ç†bug
"""
import json
import os
import random
import inspect
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Union, Dict, Any, List

import draccus
import torch
import torch.distributed as dist
from PIL import Image
import numpy as np
from torch.utils.data import Dataset, DataLoader

# é¿å…å¾ªç’°å°å…¥å•é¡Œ
from cobra.overwatch import initialize_overwatch
from cobra.util import set_global_seed

# Memory optimization
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'
torch.cuda.empty_cache()

# Single GPU setup
if "WORLD_SIZE" not in os.environ:
    os.environ["WORLD_SIZE"] = "1"
if "RANK" not in os.environ:
    os.environ["RANK"] = "0"
if "LOCAL_RANK" not in os.environ:
    os.environ["LOCAL_RANK"] = "0"

# Initialize Overwatch
overwatch = initialize_overwatch(__name__)


def create_simple_collate_fn(tokenizer, device):
    """åˆ›å»ºç®€å•çš„collateå‡½æ•°"""
    def simple_collate_fn(batch):
        try:
            # å–ç¬¬ä¸€ä¸ªitem
            item = batch[0]
            
            # ä¸ºdinosiglipåˆ›å»ºå­—å…¸æ ¼å¼çš„pixel_values
            pixel_values = item['pixel_values']
            if isinstance(pixel_values, torch.Tensor):
                if pixel_values.dim() == 3:
                    pixel_values = pixel_values.unsqueeze(0)  # [1, C, H, W]
                
                pixel_values_dict = {
                    "dino": pixel_values.to(device),
                    "siglip": pixel_values.to(device)
                }
            
            # å¤„ç†æ–‡æœ¬æ•°æ®
            input_ids = item['input_ids']
            if input_ids.dim() == 1:
                input_ids = input_ids.unsqueeze(0)
                
            attention_mask = item['attention_mask']
            if attention_mask.dim() == 1:
                attention_mask = attention_mask.unsqueeze(0)
                
            labels = item['labels']
            if labels.dim() == 1:
                labels = labels.unsqueeze(0)
            
            return {
                'pixel_values': pixel_values_dict,
                'input_ids': input_ids.to(device),
                'attention_mask': attention_mask.to(device),
                'labels': labels.to(device),
                'bbox': item['bbox'].unsqueeze(0).to(device) if item['bbox'].dim() == 1 else item['bbox'].to(device),
                'image_id': [str(item['image_id'])]
            }
            
        except Exception as e:
            overwatch.error(f"Collate error: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤batch
            return {
                'pixel_values': {
                    "dino": torch.zeros(1, 3, 384, 384).to(device),
                    "siglip": torch.zeros(1, 3, 384, 384).to(device)
                },
                'input_ids': torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long).to(device),
                'attention_mask': torch.ones(1, 5, dtype=torch.long).to(device),
                'labels': torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long).to(device),
                'bbox': torch.zeros(1, 4, dtype=torch.float32).to(device),
                'image_id': ['dummy']
            }
    
    return simple_collate_fn


class SimpleRefCOCODataset(Dataset):
    """ç®€åŒ–çš„RefCOCOæ•°æ®é›†"""
    
    def __init__(
        self,
        coco_json_path: Path,
        images_dir: Path,
        image_transform,
        tokenizer,
        split: str = "train",
        max_samples: Optional[int] = None,
        seed: int = 42
    ):
        self.coco_json_path = coco_json_path
        self.tokenizer = tokenizer
        self.max_samples = max_samples
        
        # åˆ›å»ºç®€å•çš„examples
        self.examples = self._create_simple_examples(max_samples)
        overwatch.info(f"åˆ›å»ºäº† {len(self.examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    def _create_simple_examples(self, max_samples):
        """åˆ›å»ºç®€å•çš„examples"""
        examples = []
        
        # åˆ›å»ºè™šæ‹Ÿçš„RefCOCOæ ·æœ¬
        categories = ["person", "chair", "table", "car", "dog", "cat", "book", "cup", "phone", "laptop"]
        
        for i in range(max_samples or 50):
            category = random.choice(categories)
            example = {
                "image_id": f"img_{i}",
                "expression": f"find the {category}",
                "bbox": [10.0, 10.0, 50.0, 50.0],  # è™šæ‹Ÿbbox
                "category_name": category
            }
            examples.append(example)
        
        return examples
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        """è¿”å›ç®€å•çš„æ ·æœ¬"""
        try:
            example = self.examples[idx]
            
            # åˆ›å»º384x384çš„è™šæ‹Ÿå›¾åƒï¼ˆç¬¦åˆdinosiglip-vit-so-384pxè¦æ±‚ï¼‰
            pixel_values = torch.zeros(3, 384, 384, dtype=torch.float32)
            
            # ç®€å•çš„æ–‡æœ¬tokens
            input_ids = torch.tensor([1, 2, 3, 4, 5], dtype=torch.long)
            attention_mask = torch.ones(5, dtype=torch.long)
            
            return {
                "pixel_values": pixel_values,
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "labels": input_ids.clone(),
                "bbox": torch.tensor(example["bbox"], dtype=torch.float32),
                "image_id": str(example["image_id"])
            }
            
        except Exception as e:
            overwatch.warning(f"Error in __getitem__ {idx}: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤æ ·æœ¬
            return {
                "pixel_values": torch.zeros(3, 384, 384, dtype=torch.float32),
                "input_ids": torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
                "attention_mask": torch.ones(5, dtype=torch.long),
                "labels": torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
                "bbox": torch.zeros(4, dtype=torch.float32),
                "image_id": "dummy"
            }


@dataclass
class SimpleRefCOCOTrainConfig:
    # Model configuration - ä½¿ç”¨æ ‡å‡†VLMé¿å¼€ç©ºé—´æ¨ç†é—®é¢˜
    model_id: str = "cobra+3b"
    vision_backbone_id: str = "dinosiglip-vit-so-384px"
    llm_backbone_id: str = "mamba-2.8b-zephyr"
    arch_specifier: str = "no-align+fused-gelu-mlp"
    
    # Dataset configuration
    dataset_name: str = "refcoco"
    data_root: Path = Path("data/refcoco")
    coco_json_file: str = "refcoco.json"
    split: str = "train"
    
    # Training configuration
    stage: str = "lora-finetune"
    use_lora: bool = True
    
    # LoRA configuration
    lora_rank: int = 16
    lora_alpha: float = 32.0
    lora_dropout: float = 0.1
    
    # Optimization parameters
    epochs: int = 1
    max_steps: Optional[int] = None
    global_batch_size: int = 4
    per_device_batch_size: int = 1
    learning_rate: float = 2e-4
    weight_decay: float = 0.01
    max_grad_norm: float = 1.0
    
    # Data loading
    max_samples: Optional[int] = 10
    subset_seed: int = 42
    image_resize_strategy: str = "resize-naive"
    llm_max_length: int = 512
    
    # Run configuration
    run_id: Optional[str] = None
    run_root_dir: Path = Path("runs")
    seed: int = 7
    
    # HF Hub
    hf_token: Union[str, Path] = Path(".hf_token")
    
    # Memory optimization
    enable_memory_optimization: bool = True
    gradient_accumulation_steps: int = 4
    num_workers: int = 0
    
    def __post_init__(self):
        if self.run_id is None:
            self.run_id = f"refcoco-simple+{self.stage}+samples{self.max_samples}"


def create_standard_vlm(cfg, vision_backbone, llm_backbone):
    """åˆ›å»ºæ ‡å‡†VLMï¼ˆä¸ä½¿ç”¨ç©ºé—´æ¨ç†ï¼‰"""
    try:
        overwatch.info("åˆ›å»ºæ ‡å‡†CobraVLM...")
        from cobra.models import get_vlm
        
        # æ£€æŸ¥get_vlmçš„å‡½æ•°ç­¾å
        sig = inspect.signature(get_vlm)
        params = list(sig.parameters.keys())
        overwatch.info(f"get_vlmå‚æ•°: {params}")
        
        # æ ¹æ®å‚æ•°æ•°é‡å°è¯•ä¸åŒçš„è°ƒç”¨æ–¹å¼
        if len(params) == 4:  # model_id, arch_specifier, vision_backbone, llm_backbone
            vlm = get_vlm(cfg.model_id, cfg.arch_specifier, vision_backbone, llm_backbone)
        elif len(params) == 3:  # arch_specifier, vision_backbone, llm_backbone
            vlm = get_vlm(cfg.arch_specifier, vision_backbone, llm_backbone)
        else:
            # å°è¯•æœ€å¸¸è§çš„ç»„åˆ
            vlm = get_vlm(cfg.model_id, vision_backbone, llm_backbone)
        
        overwatch.info("âœ… æ ‡å‡†VLMåˆ›å»ºæˆåŠŸ")
        return vlm
        
    except Exception as e:
        overwatch.error(f"æ ‡å‡†VLMåˆ›å»ºå¤±è´¥: {e}")
        
        # å°è¯•ç›´æ¥åˆ›å»ºCobraVLM
        try:
            overwatch.info("å°è¯•ç›´æ¥åˆ›å»ºCobraVLM...")
            from cobra.models.vlms.cobra import CobraVLM
            
            # æ£€æŸ¥CobraVLMçš„æ„é€ å‡½æ•°
            sig = inspect.signature(CobraVLM.__init__)
            params = list(sig.parameters.keys())
            overwatch.info(f"CobraVLMå‚æ•°: {params}")
            
            # æ ¹æ®å‚æ•°åˆ›å»º
            if "model_id" in params:
                vlm = CobraVLM(
                    model_id=cfg.model_id,
                    vision_backbone=vision_backbone,
                    llm_backbone=llm_backbone,
                    arch_specifier=cfg.arch_specifier
                )
            else:
                vlm = CobraVLM(
                    vision_backbone=vision_backbone,
                    llm_backbone=llm_backbone,
                    arch_specifier=cfg.arch_specifier
                )
            
            overwatch.info("âœ… ç›´æ¥CobraVLMåˆ›å»ºæˆåŠŸ")
            return vlm
            
        except Exception as e2:
            overwatch.error(f"ç›´æ¥CobraVLMåˆ›å»ºå¤±è´¥: {e2}")
            
            # æœ€åå°è¯•ï¼šä½¿ç”¨æœ€åŸºæœ¬çš„å‚æ•°
            try:
                overwatch.info("å°è¯•æœ€åŸºæœ¬çš„CobraVLMåˆ›å»º...")
                from cobra.models.vlms.cobra import CobraVLM
                
                vlm = CobraVLM(
                    vision_backbone,
                    llm_backbone
                )
                overwatch.info("âœ… æœ€åŸºæœ¬CobraVLMåˆ›å»ºæˆåŠŸ")
                return vlm
                
            except Exception as e3:
                overwatch.error(f"æœ€åŸºæœ¬CobraVLMåˆ›å»ºå¤±è´¥: {e3}")
                raise RuntimeError("æ— æ³•åˆ›å»ºä»»ä½•ç±»å‹çš„VLM")


def simple_training_step(vlm, batch, optimizer):
    """ç®€åŒ–çš„è®­ç»ƒæ­¥éª¤"""
    try:
        print("=== SIMPLE TRAINING STEP ===")
        
        # æ‰“å°è¾“å…¥ä¿¡æ¯
        print(f"pixel_values type: {type(batch['pixel_values'])}")
        if isinstance(batch['pixel_values'], dict):
            for key, value in batch['pixel_values'].items():
                print(f"  {key}: {value.shape}")
        
        print(f"input_ids: {batch['input_ids'].shape}")
        print(f"attention_mask: {batch['attention_mask'].shape}")
        print(f"labels: {batch['labels'].shape}")
        
        # æ‰§è¡Œå‰å‘ä¼ æ’­
        outputs = vlm(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            pixel_values=batch['pixel_values'],
            labels=batch['labels']
        )
        
        loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
        print(f"âœ… Loss: {loss.item()}")
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        return loss.item()
        
    except Exception as e:
        print(f"Training step error: {e}")
        import traceback
        traceback.print_exc()
        optimizer.zero_grad()
        return None


@draccus.wrap()
def simple_train_refcoco(cfg: SimpleRefCOCOTrainConfig) -> None:
    """ç®€åŒ–çš„RefCOCOè®­ç»ƒ"""
    
    overwatch.info("=== Simple RefCOCO Training ===")
    
    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    overwatch.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # Load models
    from cobra.models import get_llm_backbone_and_tokenizer, get_vision_backbone_and_transform
    
    overwatch.info("åŠ è½½æ¨¡å‹ç»„ä»¶...")
    
    vision_backbone, image_transform = get_vision_backbone_and_transform(
        cfg.vision_backbone_id, cfg.image_resize_strategy
    )
    
    llm_backbone, tokenizer = get_llm_backbone_and_tokenizer(
        cfg.llm_backbone_id,
        llm_max_length=cfg.llm_max_length,
        hf_token=""
    )
    
    # Create standard VLM (avoid spatial reasoning)
    vlm = create_standard_vlm(cfg, vision_backbone, llm_backbone)
    vlm.to(device)
    vlm.train()
    
    # Apply LoRA if requested
    if cfg.use_lora:
        overwatch.info("åº”ç”¨LoRA...")
        try:
            from cobra.util.lora_utils import apply_lora_to_linear_layers
            
            target_modules = ["mixer.in_proj", "mixer.out_proj", "mixer.x_proj", "mixer.dt_proj"]
            apply_lora_to_linear_layers(
                vlm.llm_backbone,
                target_modules=target_modules,
                rank=cfg.lora_rank,
                alpha=cfg.lora_alpha,
                dropout=cfg.lora_dropout,
            )
            overwatch.info("âœ… LoRAåº”ç”¨æˆåŠŸ")
        except Exception as e:
            overwatch.warning(f"LoRAåº”ç”¨å¤±è´¥: {e}")
    
    # Create simple dataset
    train_dataset = SimpleRefCOCODataset(
        coco_json_path=cfg.data_root / cfg.coco_json_file,
        images_dir=cfg.data_root / "images",
        image_transform=image_transform,
        tokenizer=tokenizer,
        split=cfg.split,
        max_samples=cfg.max_samples,
        seed=cfg.subset_seed
    )
    
    # Create DataLoader
    collate_fn = create_simple_collate_fn(tokenizer, device)
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=1,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )
    
    # Setup optimizer
    optimizer = torch.optim.AdamW(vlm.parameters(), lr=cfg.learning_rate)
    
    # Training loop
    overwatch.info("å¼€å§‹è®­ç»ƒ...")
    successful_steps = 0
    total_loss = 0.0
    
    for step, batch in enumerate(train_dataloader):
        if step >= 5:  # åªè®­ç»ƒ5æ­¥ä½œä¸ºæµ‹è¯•
            break
            
        loss = simple_training_step(vlm, batch, optimizer)
        
        if loss is not None:
            successful_steps += 1
            total_loss += loss
            overwatch.info(f"âœ… Step {step}, Loss: {loss:.4f}")
        else:
            overwatch.warning(f"âŒ Step {step} å¤±è´¥")
    
    # æŠ¥å‘Šç»“æœ
    if successful_steps > 0:
        avg_loss = total_loss / successful_steps
        overwatch.info(f"ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        overwatch.info(f"æˆåŠŸæ­¥æ•°: {successful_steps}")
        overwatch.info(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        try:
            save_path = cfg.run_root_dir / cfg.run_id / "final_model.pt"
            save_path.parent.mkdir(parents=True, exist_ok=True)
            torch.save({
                'model_state_dict': vlm.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'config': cfg,
                'successful_steps': successful_steps,
                'avg_loss': avg_loss
            }, save_path)
            overwatch.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        except Exception as e:
            overwatch.warning(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    else:
        overwatch.warning("âŒ æ²¡æœ‰æˆåŠŸçš„è®­ç»ƒæ­¥éª¤")


if __name__ == "__main__":
    simple_train_refcoco()